---
title: 6.File System 管理
weight: 1
---

# 概述

> 参考：
> - [Wiki-Category,Computer file systemd](https://en.wikipedia.org/wiki/Category:Computer_file_systems)
> - [Linux 性能优化实践-文件系统](https://time.geekbang.org/column/article/76876)
> - [公众号，小林 coding-一口气搞懂「文件系统」，就靠这 25 张图了](https://mp.weixin.qq.com/s/qJdoXTv_XS_4ts9YuzMNIw)

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167805545-2b948cff-7e56-4eb8-8c12-3851fd6c2e36.png)

> 图片来源：<https://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram>
> 从上面的结构可以看到，文件系统的作用就是用来接收用户的操作，并将数据保存到物理硬盘的。可以想见，如果没有文件系统帮助用户操作，那么人们又怎么能将数据保存到存储设备上呢~

**File System(文件系统，简称 FS)** 是一种对存储设备上的数据，进行组织管理的机制。组织方式的不同，就会形成不同的文件系统
如果没有文件系统，放置在存储介质中的数据将是一个庞大的数据主体，无法分辨一个数据在哪里停止以及下一个数据在哪里开始。通过将数据分成多个部分并给每个部分命名，可以轻松地隔离和识别数据。每组数据称为 **File(文件)**。所以，用于管理这些文件及其名称的**结构和逻辑规则**，称为 **File System(文件系统)**。

## 什么是 File(文件)

详见《[文件管理](/docs/IT学习笔记/1.操作系统/2.Kernel(内核)/6.File_System_管理/文件管理/文件管理.md)》章节

# 文件组织结构

> 文件管理详解见[单独章节](/docs/IT学习笔记/1.操作系统/2.Kernel(内核)/6.File_System_管理/文件管理/文件管理.md)

为了方便管理，Linux 的文件系统为每个文件都分配了两个数据结构。

- **index node(索引节点，简称 inode)** # 记录文件的元数据。inode 编号、文件大小、访问权限、修改日期、数据的位置等。
  - inode 和文件一一对应，它跟文件内容一样，都会被持久化到存储的磁盘中。所以** inode 同样占用磁盘空间**。
  - inode 包含文件的元数据，具体来说有以下内容：
    - 文件的字节数
    - 文件拥有者的 User ID
    - 文件的 Group ID
    - 文件的读、写、执行权限
    - 文件的时间戳，共有三个：ctime 指 inode 上一次变动的时间，mtime 指文件内容上一次变动的时间，atime 指文件上一次打开的时间。
    - 链接数，即有多少文件名指向这个 inode
    - 文件数据 block 的位置
- **directory entry(目录项，简称 dentry)** # 记录文件的名字、inode 指针、与其他目录项的关联关系。
  - 多个关联的目录项，就构成了文件系统的目录结构(**一个层次化的树形结构**)。不过，不同于 inode，目录项是由内核维护的一个内存数据结构，所以通常也被叫做 **dentries(目录项缓存)。**

这个层次化的树形结构就像下图一样：
![image.png](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1617088781476-3d7a9ccc-e8df-4680-acc5-26f4f82aa8b5.png)
**注意：目录项缓存记录在 slab 中，当我们使用 find 命令时，slab 中的 dentry 缓存就会增大；打开文件过多，slab 中的 dentry 缓存也会增大。**
inode 是每个文件的唯一标志，而 dentry 维护的正是文件系统的树状结构。dentry 与 inode 的关系是多对一(可以简单理解为一个文件可以有多个别名)
下面用一个形象点的白话来描述这些概念，假如现在系统中有如下目录结构

```bash
~]# tree --inodes
.
├── [   2218]  dir_1 # 这是目录类型的文件
│   ├── [   2235]  file_1 # 这是普通类型的文件
│   └── [   2236]  file_2
├── [269167463]  dir_2
│   └── [269167464]  file_3
└── [537384536]  dir_3
    ├── [   2235]  fie_1_ln
    └── [537384537]  file_4
3 directories, 5 files
```

可以这么描述上述看到的内容：dir_1、file_1、dir_2 这些名称都是 dentry 中的文件名，`[]` 中的数字是 inode 号，每个 dentry 都会与 inode 关联。其中 file_1 和 file_1_ln 的 inode 相同，但是 dentry 不同，这就对应了 dentry 与 inode 是多对一的关系。而哪些文件在哪个目录中，则是由每个文件的 dentry 中的关联关系来决定。比如 dir_1 目录中，包含了 file_1 和 file_2 文件。

> 索引节点和目录项记录了文件的元数据，以及文件间的目录关系，那么具体来说，文件数据到底是怎么存储的呢？是不是直接写到磁盘中就好了呢？
> 实际上，磁盘读写的最小单位是扇区，然而扇区只有 512B 大小，如果每次都读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成了逻辑块，然后每次都以逻辑块为最小单元，来管理数据。常见的逻辑块大小为 4KB，也就是由连续的 8 个扇区组成。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167805558-180916c2-cc19-40a0-b8f4-3ff805929883.png)
注意：

- dentry 本身只是一个存储在内存中的缓存，而 inode 则是存储在磁盘中的数据。由于内存的 Buffer 和 Cache 原理，所以 inode 也会缓存到内存中，以便加速文件的访问。
- 磁盘在执行文件系统格式化时，会被分成三个存储区域，超级快、索引节点区、数据区块
  - 超级块 # 存储整个文件系统的状态
  - 索引节点区 # 存储 inode
  - 数据区块 # 存储文件数据
- 我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使用的时候，才将其加载进内存，它们加载进内存的时机是不同的：
  - 超级块 # 当文件系统挂载时进入内存；
  - 索引节点区 # 当文件被访问时进入内存

dentry、inode、逻辑块以及超级块构成了 Linux 文件系统的四大基本要素。

## 小结

通过上面的描述，文件在文件系统中，也就可以归纳为两个部分

- 指针部分 # 指针位于文件系统的元数据中，在将数据删除后，这个指针就从元数据中清除了(元数据其实就是上文的 inode 与 dentry)。
- 数据部分 # 文件的具体内容，存储在磁盘中。

平时我们在删除数据时，其实仅仅从元数据中删除了数据对应的指针。当指针被删除时，其原本占用的空间就可以被覆盖并写入新内容。

## 常见问题

- 这也是为什么我们可以恢复数据的原因，只要旧数据还没被覆盖，就依然可以获取到。
- 有时候在删除文件时，会发现并没有释放空间，也是同样的道理，当某个进程持续写入内容时，如果强制删除了文件，由于进程锁定文件对应的指针部分并不会从元数据中清除，而由于指针并未删除，系统内核就默认文件并未删除，因此查询文件系统空间时，显示空间并未释放。可以通过 lsof 命令筛选 deleted 查找这些有问题的文件。

# Virtual File System(虚拟文件系统)

> 参考：
> - [Wiki,Virtual file system](https://en.wikipedia.org/wiki/Virtual_file_system)
> - [知乎](https://zhuanlan.zhihu.com/p/69289429)
> - [极客-Linux 性能优化实践](https://time.geekbang.org/column/article/76876)

**Virtual File System(虚拟文件系统，简称 VFS)**。是 Linux 为了支持多种多样的文件系统，在用户空间进程和文件系统中间，引入的一个抽象层。VFS 的目的是运行客户端应用程序以统一的方式访问不同类型的文件系统。VFS **定义了**一组所有文件系统都支持的**数据结构和标准 API**。这样，用户进程和内核中的其他子系统，只需要跟 VFS 提供的统一接口进行交互即可，而不需要关系底层各种文件系统的实现细节。

> 比如不同文件系统的调用函数不一样，如果没有 VFS ，那么在使用的时候，就需要为特定的文件系统，编写不同的调用方式，非常繁琐复杂。
> 比如 VFS 可以用来弥合 Windows，经典 Mac OS / macOS 和 Unix 文件系统中的差异，以便应用程序可以访问那些类型的本地文件系统上的文件，而不必知道它们正在访问哪种文件系统。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167805621-09dbf293-4f9a-4892-8e30-8d33f32031c4.png)

举个例子，Linux 用户程序可以通过`read()` 来读取`ext4`、`NFS`、`XFS`等文件系统的文件，也可以读取存储在`SSD`、`HDD`等不同存储介质的文件，无须考虑不同文件系统或者不同存储介质的差异。

通过 VFS 系统，Linux 提供了通用的系统调用，可以跨越不同文件系统和介质之间执行，极大简化了用户访问不同文件系统的过程。另一方面，新的文件系统、新类型的存储介质，可以无须编译的情况下，动态加载到 Linux 中。

"一切皆文件"是 Linux 的基本哲学之一，不仅是普通的文件，包括目录、字符设备、块设备、套接字等，都可以以文件的方式被对待。实现这一行为的基础，正是 Linux 的虚拟文件系统机制。

VFS 之所以能够衔接各种各样的文件系统，是因为它抽象了一个通用的文件系统模型，定义了通用文件系统都支持的、概念上的接口。新的文件系统只要支持并实现这些接口，并注册到 Linux 内核中，即可安装和使用。

再举个例子，比如 Linux 写一个文件：

```bash
int ret = write(fd, buf, len);
```

调用了`write()`系统调用，它的过程简要如下：

- 首先，勾起 VFS 通用系统调用`sys_write()`处理。
- 接着，`sys_write()`根据`fd`找到所在的文件系统提供的写操作函数，比如`op_write()`。
- 最后，调用`op_write()`实际的把数据写入到文件中。

操作示意图如下：
![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167805551-1b23e389-6142-4e11-8ef1-b1b1c1722cbe.jpeg)

# 文件系统类型

> 参考：
> - [Wiki,File system-Types_of_file_systems](https://en.wikipedia.org/wiki/File_system#Types_of_file_systems)

可以通过 **/proc/filesystems** 文件查看当前内核所支持的文件系统类型

```bash
~]# cat /proc/filesystems
nodev	sysfs
...
nodev	proc
...
nodev	cgroup2
nodev	tmpfs
nodev	devtmpfs
nodev	configfs
...
	    xfs
...
	    ext4
...
```

- 第一列说明文件系统是否需要挂载在一个块设备上
  - nodev 表明本行的文件系统类型不需要挂接在块设备上。
- 第二列是内核支持的文件系统类型。

当系统中安装了某个文件系统的驱动，则该文件内容也会有增加，比如我安装了 nfs-utils 包，则该文件还会增加 nfs 行。

## 按照存储位置的不同，这些文件系统可以大体分为如下几类

- **Disk file systems(磁盘文件系统)**
  - 基于磁盘的文件系统，也就是把数据直接存储 计算机本地挂载磁盘中。常见的** ext4、xfs、overlayFS** 等，都是这类文件系统
- **Network File Systems(网络文件系统)**
  - 网络文件系统是充当远程文件访问协议的客户端的文件系统，提供对服务器上文件的访问。 使用本地接口的程序可以透明地创建，管理和访问远程网络连接计算机中的分层目录和文件。 网络文件系统的示例包括 NFS，AFS，SMB 协议的客户端，以及 FTP 和 WebDAV 的类似于文件系统的客户端。
- **Distributed File System(分布式文件系统)** # 使用网络协议的分布式文件系统也属于网络文件系统的一种。
- **Special-purpose File Systems(特殊目的文件系统)** # 特殊的文件系统将操作系统的非文件元素显示为文件，以便可以使用文件系统 API 对其进行操作。 这种文件系统一般都是基于内存的，不需要任何磁盘为其分配存储空间，但会占用内存。
  - **device file system(设备文件系统)** # 简称 devfs，设备文件系统将 I/O 设备和伪设备表示为文件，称为设备文件。 默认挂载到`/dev`目录下。
  - **Proc File System(进程文件系统)** # 简称\_ \_procfs，将进程以及 Linux 上的其他操作系统结构映射到文件空间。默认挂载到`/proc`目录下。
  - **configfs** 和 **sysfs** 提供了可用于向内核查询信息并在内核中配置实体的文件。
  - 等等

# 文件系统的使用

> 和 DOS 等操作系统不同，Linux 操作系统中文件系统并不是由驱动器号或驱动器名称（如 A: 或 C: 等）来标识的。Linux 操作系统将独立的文件系统组合成了一个层次化的树形结构，并且由一个单独的实体代表这一文件系统。

Linux 将新的文件系统通过一个称为 **Mount(挂载)** 的操作将其挂载到某个目录上，从而让不同的文件系统结合成为一个整体。
这些文件系统，要先挂载到 VFS 目录树中的某个子目录（称为挂载点），然后才能访问其中的文件。拿第一类，也就是基于磁盘的文件系统为例，在安装系统时，要先挂载一个根目录`/`，在根目录下再把其他文件系统（比如其他的磁盘分区、/proc 文件系统、/sys 文件系统、NFS 等）挂载进来。

## 文件系统 I/O

把文件系统挂载到挂载点后，就可以通过挂载点访问它管理的文件了。 VFS 提供了一组标准的文件访问接口。这些接口以系统调用的方式，提供给应用程序使用。
就比如 cat 命令，首先调用 `openat()` 打开一个文件，然后调用 `read()` 读取文件内容，最后调用 `write()` 将内容输出到控制台的标准输出中

```bash
~]# strace -e openat,read,write cat /root/test
......
openat(AT_FDCWD, "/root/test", O_RDONLY) = 3
read(3, "Test I/O for File System", 131072) = 24
write(1, "Test I/O for File System", 24Test I/O for File System) = 24
......
# 代码中的方法如下：open() 与 openat() 这两个调用效果一样。
int open(const char *pathname, int flags, mode_t mode);
ssize_t read(int fd, void *buf, size_t count);
ssize_t write(int fd, const void *buf, size_t count);

```

文件读写方式的各种差异，导致 I/O 的分类多种多样。最常见的有，缓冲与非缓冲 I/O、直接与非直接 I/O、阻塞与非阻塞 I/O、同步与异步 I/O 等。

### 缓冲与非缓冲 I/O

根据是否利用标准库缓存

- 缓冲 I/O # 利用标准库缓存来加速文件的访问，而标准库内部再通过系统调用访问文件
- 非缓冲 I/O # 直接通过系统调用来访问文件，不再经过标准库缓存。

注意，这里所说的“缓冲”，是指标准库内部实现的缓存。比方说，你可能见到过，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来。
无论缓冲 I/O 还是非缓冲 I/O，它们最终还是要经过系统调用来访问文件。而根据上一节内容，我们知道，系统调用后，还会通过页缓存，来减少磁盘的 I/O 操作。

### 直接与非直接 I/O

根据是否利用操作系统的页缓存

1. 直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。
2. 非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘。

想要实现直接 I/O，需要你在系统调用中，指定 O_DIRECT 标志。如果没有设置过，默认的是非直接 I/O。
不过要注意，直接 I/O、非直接 I/O，本质上还是和文件系统交互。如果是在数据库等场景中，你还会看到，跳过文件系统读写磁盘的情况，也就是我们通常所说的裸 I/O。

### 阻塞与非阻塞 I/O

根据应用程序是否阻塞自身运行

1. 所谓阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务。
2. 所谓非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果。

比方说，访问管道或者网络套接字时，设置 O_NONBLOCK 标志，就表示用非阻塞方式访问；而如果不做任何设置，默认的就是阻塞访问。

### 同步与异步 I/O

根据是否等待响应结果

1. 所谓同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得 I/O 响应。
2. 所谓异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序。

举个例子，在操作文件时，如果你设置了 O_SYNC 或者 O_DSYNC 标志，就代表同步 I/O。如果设置了 O_DSYNC，就要等文件数据写入磁盘后，才能返回；而 O_SYNC，则是在 O_DSYNC 基础上，要求文件元数据也要写入磁盘后，才能返回。
再比如，在访问管道或者网络套接字时，设置了 O_ASYNC 选项后，相应的 I/O 就是异步 I/O。这样，内核会再通过 SIGIO 或者 SIGPOLL，来通知进程文件是否可读写。
你可能发现了，这里的好多概念也经常出现在网络编程中。比如非阻塞 I/O，通常会跟 select/poll 配合，用在网络套接字的 I/O 中。
你也应该可以理解，“Linux 一切皆文件”的深刻含义。无论是普通文件和块设备、还是网络套接字和管道等，它们都通过统一的 VFS 接口来访问。

## 总结

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677139-656d34bf-0195-4576-919f-2eedc4f4ba5a.png)
在前面我们知道了，I/O 是分为两个过程的：

1. 数据准备的过程
2. 数据从内核空间拷贝到用户进程缓冲区的过程

阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，而非阻塞 I/O 和基于非阻塞 I/O 的多路复用只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。
异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。

## 用故事去理解这几种 I/O 模型

举个你去饭堂吃饭的例子，你好比用户程序，饭堂好比操作系统。
阻塞 I/O 好比，你去饭堂吃饭，但是饭堂的菜还没做好，然后你就一直在那里等啊等，等了好长一段时间终于等到饭堂阿姨把菜端了出来（数据准备的过程），但是你还得继续等阿姨把菜（内核空间）打到你的饭盒里（用户空间），经历完这两个过程，你才可以离开。
非阻塞 I/O 好比，你去了饭堂，问阿姨菜做好了没有，阿姨告诉你没，你就离开了，过几十分钟，你又来饭堂问阿姨，阿姨说做好了，于是阿姨帮你把菜打到你的饭盒里，这个过程你是得等待的。
基于非阻塞的 I/O 多路复用好比，你去饭堂吃饭，发现有一排窗口，饭堂阿姨告诉你这些窗口都还没做好菜，等做好了再通知你，于是等啊等（`select` 调用中），过了一会阿姨通知你菜做好了，但是不知道哪个窗口的菜做好了，你自己看吧。于是你只能一个一个窗口去确认，后面发现 5 号窗口菜做好了，于是你让 5 号窗口的阿姨帮你打菜到饭盒里，这个打菜的过程你是要等待的，虽然时间不长。打完菜后，你自然就可以离开了。
异步 I/O 好比，你让饭堂阿姨将菜做好并把菜打到饭盒里后，把饭盒送到你面前，整个过程你都不需要任何等待。

# 文件的存储

文件的数据是要存储在硬盘上面的，数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种：

- 连续空间存放方式
- 非连续空间存放方式

其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。
不同的存储方式，有各自的特点，重点是要分析它们的存储效率和读写性能，接下来分别对每种存储方式说一下。

## 连续空间存放方式

> 注意：这里只针对机械硬盘，固态硬盘并没有磁道等概念

连续空间存放方式顾名思义，**文件存放在磁盘「连续的」物理空间中**。这种模式下，文件的数据都是紧密相连，**读写效率很高**，因为一次磁盘寻道就可以读出整个文件。

使用连续存放的方式有一个前提，必须先知道一个文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。

所以，**文件头里需要指定「起始块的位置」和「长度」**，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。

注意，此处说的文件头，就类似于 Linux 的 inode。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677049-26c7ae42-9e37-426a-99f9-6e59df62e691.png)

连续空间存放的方式虽然读写效率高，**但是有「磁盘空间碎片」和「文件长度不易扩展」的缺陷。**

如下图，如果文件 B 被删除，磁盘上就留下一块空缺，这时，如果新来的文件小于其中的一个空缺，我们就可以将其放在相应空缺里。但如果该文件的大小大于所有的空缺，但却小于空缺大小之和，则虽然磁盘上有足够的空缺，但该文件还是不能存放。当然了，我们可以通过将现有文件进行挪动来腾出空间以容纳新的文件，但是这个在磁盘挪动文件是非常耗时，所以这种方式不太现实。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677046-4fd1191e-4e74-4653-8ef4-24ba781c4f57.png)

另外一个缺陷是文件长度扩展不方便，例如上图中的文件 A 要想扩大一下，需要更多的磁盘空间，唯一的办法就只能是挪动的方式，前面也说了，这种方式效率是非常低的。
那么有没有更好的方式来解决上面的问题呢？答案当然有，既然连续空间存放的方式不太行，那么我们就改变存放的方式，使用非连续空间存放方式来解决这些缺陷。

## 非连续空间存放方式

非连续空间存放方式分为「链表方式」和「索引方式」。

> 我们先来看看链表的方式。

链表的方式存放是**离散的，不用连续的**，于是就可以**消除磁盘碎片**，可大大提高磁盘空间的利用率，同时**文件的长度可以动态扩展**。根据实现的方式的不同，链表可分为「**隐式链表**」和「**显式链接**」两种形式。

文件要以「**隐式链表**」的方式存放的话，**实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置**，这样一个数据块连着一个数据块，从链头开是就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。
![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677039-c71df217-651d-42ac-8cf6-444dedae8c1c.png)
隐式链表的存放方式的**缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间**。隐式链接分配的**稳定性较差**，系统在运行过程中由于软件或者硬件错误**导致链表中的指针丢失或损坏，会导致文件数据的丢失。**

如果取出每个磁盘块的指针，把它放在内存的一个表中，就可以解决上述隐式链表的两个不足。那么，这种实现方式是「**显式链接**」，它指**把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中**，该表在整个磁盘仅设置一张，**每个表项中存放链接指针，指向下一个数据块号**。

对于显式链接的工作方式，我们举个例子，文件 A 依次使用了磁盘块 4、7、2、10 和 12 ，文件 B 依次使用了磁盘块 6、3、11 和 14 。利用下图中的表，可以从第 4 块开始，顺着链走到最后，找到文件 A 的全部磁盘块。同样，从第 6 块开始，顺着链走到最后，也能够找出文件 B 的全部磁盘块。最后，这两个链都以一个不属于有效磁盘编号的特殊标记（如 -1 ）结束。内存中的这样一个表格称为**文件分配表（File Allocation Table，FAT）**。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677063-b7bb2a32-dde6-4c17-9119-096413902ae8.png)

由于查找记录的过程是在内存中进行的，因而不仅显著地**提高了检索速度**，而且**大大减少了访问磁盘的次数**。但也正是整个表都存放在内存中的关系，它的主要的缺点是**不适用于大磁盘\*\*。

比如，对于 200GB 的磁盘和 1KB 大小的块，这张表需要有 2 亿项，每一项对应于这 2 亿个磁盘块中的一个块，每项如果需要 4 个字节，那这张表要占用 800MB 内存，很显然 FAT 方案对于大磁盘而言不太合适。

> 接下来，我们来看看索引的方式。

链表的方式解决了连续分配的磁盘碎片和文件动态扩展的问题，但是不能有效支持直接访问（FAT 除外），索引的方式可以解决这个问题。
索引的实现是为每个文件创建一个「**索引数据块**」，里面存放的是**指向文件数据块的指针列表**，说白了就像书的目录一样，要找哪个章节的内容，看目录查就可以。

另外，**文件头需要包含指向「索引数据块」的指针**，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。
创建文件时，索引块的所有指针都设为空。当首次写入第 i 块时，先从空闲空间中取得一个块，再将其地址写到索引块的第 i 个条目。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677054-825db739-3c6c-4fec-bb21-1dd52effb539.png)

索引的方式优点在于：

- 文件的创建、增大、缩小很方便；
- 不会有碎片的问题；
- 支持顺序读写和随机读写；

由于索引数据也是存放在磁盘块的，如果文件很小，明明只需一块就可以存放的下，但还是需要额外分配一块来存放索引数据，所以缺陷之一就是存储索引带来的开销。

如果文件很大，大到一个索引数据块放不下索引信息，这时又要如何处理大文件的存放呢？我们可以通过组合的方式，来处理大文件的存。

先来看看链表 + 索引的组合，这种组合称为「**链式索引块**」，它的实现方式是**在索引数据块留出一个存放下一个索引数据块的指针**，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。那这种方式也会出现前面提到的链表方式的问题，万一某个指针损坏了，后面的数据也就会无法读取了。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677072-417dcaa4-348d-4a45-b9b0-aa170d0f3bd7.png)

还有另外一种组合方式是索引 + 索引的方式，这种组合称为「**多级索引块**」，实现方式是**通过一个索引块来存放多个索引数据块**，一层套一层索引，像极了俄罗斯套娃是吧。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677056-ecb3b996-c14c-455d-82aa-7ca16aeb7feb.png)

## Unix 文件的实现方式

我们先把前面提到的文件实现方式，做个比较：

![image.png](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1671074085764-374b9218-86e8-412d-85c7-9d051b9340b2.png)

那早期 Unix 文件系统是组合了前面的文件存放方式的优点，如下图：

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677095-e05fe455-c302-43bb-a9d8-dd43cd4551e1.png)

它是根据文件的大小，存放的方式会有所变化：

- 如果存放文件所需的数据块小于 10 块，则采用直接查找的方式；
- 如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式；
- 如果前面两种方式都不够存放大文件，则采用二级间接索引方式；
- 如果二级间接索引也不够存放大文件，这采用三级间接索引方式；

那么，文件头（_Inode_）就需要包含 13 个指针：

- 10 个指向数据块的指针；
- 第 11 个指向索引块的指针；
- 第 12 个指向二级索引块的指针；
- 第 13 个指向三级索引块的指针；

所以，这种方式能很灵活地支持小文件和大文件的存放：

- 对于小文件使用直接查找的方式可减少索引数据块的开销；
- 对于大文件则以多级索引的方式来支持，所以大文件在访问数据块时需要大量查询；

这个方案就用在了 Linux Ext 2/3 文件系统里，虽然解决大文件的存储，但是对于大文件的访问，需要大量的查询，效率比较低。

为了解决这个问题，Ext 4 做了一定的改变，具体怎么解决的，本文就不展开了。

# 空闲空间管理

前面说到的文件的存储是针对已经被占用的数据块组织和管理，接下来的问题是，如果我要保存一个数据块，我应该放在硬盘上的哪个位置呢？难道需要将所有的块扫描一遍，找个空的地方随便放吗？

那这种方式效率就太低了，所以针对磁盘的空闲空间也是要引入管理的机制，接下来介绍几种常见的方法：

- 空闲表法
- 空闲链表法
- 位图法

## 空闲表法

空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图：
![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677079-9e2989d6-2cda-4460-80ca-f3c0f28783f1.png)
当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。
这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。

## 空闲链表法

我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图：
![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677070-be98384a-ad4f-4a20-b5da-1d969816b938.png)
当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。

这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。

空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。

## 位图法

位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。

当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：

    1111110011111110001110110111111100111 ...

在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。

# 文件系统的结构

前面提到 Linux 是用位图的方式管理空闲空间，用户在创建一个新文件时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要存储数据时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的。

数据块的位图是放在磁盘块里的，假设是放在一个块里，一个块 4K，每位表示一个数据块，共可以表示 `4 * 1024 * 8 = 2^15` 个空闲块，由于 1 个数据块是 4K 大小，那么最大可以表示的空间为 `2^15 * 4 * 1024 = 2^27` 个 byte，也就是 128M。

也就是说按照上面的结构，如果采用「一个块的位图 + 一系列的块」，外加「一个块的 inode 的位图 + 一系列的 inode 的结构」能表示的最大空间也就 128M，这太少了，现在很多文件都比这个大。

在 Linux 文件系统，把这个结构称为一个**块组**，那么有 N 多的块组，就能够表示 N 大的文件。

下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布：

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677099-7ffa20c4-57d9-4c49-9e02-24afef066cfb.png)

最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下：

- **超级块** # 包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。
- **块组描述符** # 包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。
- **数据位图和 inode 位图** # 用于表示对应的数据块或 inode 是空闲的，还是被使用中。
- **inode 列表** # 包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。
- **数据块** # 包含文件的有用数据。

你可以会发现每个块组里有很多重复的信息，比如**超级块和块组描述符表，这两个都是全局信息，而且非常的重要**，这么做是有两个原因：

- 如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。
- 通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。

不过，Ext2 的后续版本采用了稀疏技术。该做法是，超级块和块组描述符表不再存储到文件系统的每个块组中，而是只写入到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中。
