---
title: "EXT FileSystem"
linkTitle: "EXT FileSystem"
weight: 1
---

# 概述

> 参考：
> 
> - [公众号，小林 coding-一口气搞懂「文件系统」，就靠这 25 张图了](https://mp.weixin.qq.com/s/qJdoXTv_XS_4ts9YuzMNIw)
> - [骏马金龙，第4章 ext文件系统机制原理剖析](https://www.junmajinlong.com/linux/ext_filesystem/)

硬盘最底层的读写 IO 一次是一个扇区 512 字节，如果要读写大量文件，以扇区为单位肯定很慢很消耗性能，所以硬盘使用了一个称作逻辑块的概念。逻辑块是逻辑的，由磁盘驱动器负责维护和操作，它并非是像扇区一样物理划分的。一个逻辑块的大小可能包含一个或多个扇区，每个逻辑块都有唯一的地址，称为 LBA。有了逻辑块之后，磁盘控制器对数据的操作就以逻辑块为单位，一次读写一个逻辑块，磁盘控制器知道如何将逻辑块翻译成对应的扇区并读写数据。

到了 Linux 操作系统层次，通过文件系统提供了一个也称为块的读写单元，文件系统数据块的大小一般为 1024bytes (1KiB) 或 2048bytes (2KiB) 或 4096bytes (4KiB)。文件系统数据块也是逻辑概念，是文件系统层次维护的，而磁盘上的逻辑数据块是由磁盘控制器维护的，文件系统的 IO 管理器知道如何将它的数据块翻译成磁盘维护的数据块地址 LBA。

对于使用文件系统的 IO 操作来说，比如读写文件，这些 **IO 的基本单元**是**文件系统上的数据块**，一次读写一个文件系统数据块。比如需要读一个或多个块时，文件系统的 IO 管理器首先计算这些文件系统块对应在哪些磁盘数据块，也就是计算出 LBA，然后通知磁盘控制器要读取哪些块的数据，硬盘控制器将这些块翻译成扇区地址，然后从扇区中读取数据，再通过硬盘控制器将这些扇区数据重组写入到内存中去。

**Block(逻辑块)** 简称 块，存放数据的最小单位，假如每个块为 4KiB，那大于 5KiB 的块就需要两个块，并且逻辑上占用了 8KiB 的空间。

**Block Group(逻辑块组)** 简称块组，多个 Block 的集合

Ext 预留了一些 Inode 做特殊特性使用，如下：某些可能并非总是准确，具体的 inode 号对应什么文件可以使用 `find /-inum NUM` 查看。
```bash
Ext4的特殊inode
Inode号    用途
0         不存在0号inode，可用于标识目录data block中已删除的文件
1         虚拟文件系统，如/proc和/sys
2         根目录         # 注意此行
3         ACL索引
4         ACL数据
5         Boot loader
6         未删除的目录
7         预留的块组描述符inode
8         日志inode
11        第一个非预留的inode，通常是 lost+found 目录
```

所以在 ext4 文件系统的 dumpe2fs 信息中，能观察到 fisrt inode 号可能为 11 也可能为 12。

## 块、块组、Inode 计算

> 参考：
> - 参考哪里？我也想知道真实的计算逻辑。。。

这些计算的结果通常与下列设置有关

- **DiskSize** # 磁盘空间
- **BlockSize** # 通常为 4096 Bytes
    - 可使用 mke2fs -b 手动指定
- **BlocksPerGroup** # 通常为 32768。每个块组中块的数量。	- 可使用 mke2fs -g 手动指定
- **BytesPerInode** # 通常为 16384 Bytes。创建文件系统时，为每块 BytesPerInode 大小的空间创建一个 Inode。
    - BytesPerInode 也称为 inode_ratio，即.Inode 比率，全称应该是 Inode 分配比率，即每多少空间分配一个 Inode。
    - 可使用 mke2fs -i 手动指定 
- **InodeSize** # 通常为 256 Bytes。大小是 128 的倍数，最小为 128 Bytes。 

其中 BlocksPerGroup(每个块组中块的数量)、BytesPerInode(每个Inode负责的空间大小) 这种值是后续计算的基础。固定下来这些，就算分区空间自动扩容/缩容，也可以根据这种数据自动增加/删除块的数量和 Inode 的数量。

将会计算出
- **BlockCount** # 块总数
- **InodeCount** # Inode 总数
- **InodePreGroup** # 每个块组中包含的 Inode 数量
- **InodeUseage** # 所有 Inode 占用的空间

假如现在有一块 35GiB 的磁盘，需要先转为 Bytes。然后根据给定的 BlockSize(块大小) 和 BlocksPerGroup(块组中块的数量)，计算出需要创建创建的**块数量**和**块组数量**。

**Block 与 BlockGroup 的计算**

- BlockCount = DiskSize / BlockSize = 37580963840 / 4096 = 9175040
- BlockGroupCount = BlockCount / BlocksPerGroup = 9175040 / 32768 = 280

**Inode 的计算**

- InodeCount = DiskSize / BytesPerInode = 37580963840 / 16384 = 2293760

计算出的 Inode 数量将会平均分配到每个块组中

- InodePreGroup = InodeCount / BlockGroupCount = 2293760 / 280 = 8192

计算所有 Inode 需要占用的磁盘空间

- InodeUseage = InodeCount * InodeSize = 2293760 * 256 = 587202560 Bytes = 560 MiB

也就是说，一块 35 G 的硬盘，需要拿出来至少 560 MiB 的空间来存放 Inode 数据。

**TODO:**
- **Inode 还有一个最低数量？就算在 mke2fs 中使用 -i 指定了跟 DiskSize 相同的大小(或者  -N 1)，最后也不会只有一个 Inode，而是有 4480 个 Inode，这个数是怎么来的？**

上述计算的结果可以通过 dumpe2fs 命令获得

```bash
~]# dumpe2fs -h ${DEVICE} | egrep -i "block|inode"
dumpe2fs 1.45.5 (07-Jan-2020)
Filesystem features:      ext_attr resize_inode dir_index filetype sparse_super large_file
Inode count:              2293760
Block count:              9175040
Reserved block count:     458752
Free blocks:              9018814
Free inodes:              2293749
First block:              0
Block size:               4096
Reserved GDT blocks:      1021
Blocks per group:         32768
Inodes per group:         8192
Inode blocks per group:   512
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:	          256
```

### 最低的 Inode

假如我们需要最少 1 个 Inode，那首先根据 BlockGroupCount 的数量决定至少应该需要 280 Inode，

```bash
~]# mke2fs -N 1 /dev/vdb
mke2fs 1.45.5 (07-Jan-2020)
/dev/vdb contains a ext2 file system
	last mounted on Sat Mar 11 16:14:14 2023
Proceed anyway? (y,N) y
Creating filesystem with 9175040 4k blocks and 4480 inodes
Filesystem UUID: acebc9ab-c53e-4f74-bd6b-443343a76bab
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624

Allocating group tables: done                            
Writing inode tables: done                            
Writing superblocks and filesystem accounting information: done

~]# dumpe2fs -h ${DEVICE} | egrep -i "block|inode"
dumpe2fs 1.45.5 (07-Jan-2020)
Filesystem features:      ext_attr resize_inode dir_index filetype sparse_super large_file
Inode count:              4480
Block count:              9175040
Reserved block count:     458752
Free blocks:              9161894
Free inodes:              4469
First block:              0
Block size:               4096
Reserved GDT blocks:      1021
Blocks per group:         32768
Inodes per group:         16
Inode blocks per group:   1
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:	          256
```

# 文件的存储

文件的数据是要存储在硬盘上面的，数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种：

- 连续空间存放方式
- 非连续空间存放方式

其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。
不同的存储方式，有各自的特点，重点是要分析它们的存储效率和读写性能，接下来分别对每种存储方式说一下。

## 连续空间存放方式

> 注意：这里只针对机械硬盘，固态硬盘并没有磁道等概念

连续空间存放方式顾名思义，**文件存放在磁盘「连续的」物理空间中**。这种模式下，文件的数据都是紧密相连，**读写效率很高**，因为一次磁盘寻道就可以读出整个文件。

使用连续存放的方式有一个前提，必须先知道一个文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。

所以，**文件头里需要指定「起始块的位置」和「长度」**，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。

注意，此处说的文件头，就类似于 Linux 的 inode。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677049-26c7ae42-9e37-426a-99f9-6e59df62e691.png)

连续空间存放的方式虽然读写效率高，**但是有「磁盘空间碎片」和「文件长度不易扩展」的缺陷。**

如下图，如果文件 B 被删除，磁盘上就留下一块空缺，这时，如果新来的文件小于其中的一个空缺，我们就可以将其放在相应空缺里。但如果该文件的大小大于所有的空缺，但却小于空缺大小之和，则虽然磁盘上有足够的空缺，但该文件还是不能存放。当然了，我们可以通过将现有文件进行挪动来腾出空间以容纳新的文件，但是这个在磁盘挪动文件是非常耗时，所以这种方式不太现实。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677046-4fd1191e-4e74-4653-8ef4-24ba781c4f57.png)

另外一个缺陷是文件长度扩展不方便，例如上图中的文件 A 要想扩大一下，需要更多的磁盘空间，唯一的办法就只能是挪动的方式，前面也说了，这种方式效率是非常低的。
那么有没有更好的方式来解决上面的问题呢？答案当然有，既然连续空间存放的方式不太行，那么我们就改变存放的方式，使用非连续空间存放方式来解决这些缺陷。

## 非连续空间存放方式

非连续空间存放方式分为「链表方式」和「索引方式」。

> 我们先来看看链表的方式。

链表的方式存放是**离散的，不用连续的**，于是就可以**消除磁盘碎片**，可大大提高磁盘空间的利用率，同时**文件的长度可以动态扩展**。根据实现的方式的不同，链表可分为「**隐式链表**」和「**显式链接**」两种形式。

文件要以「**隐式链表**」的方式存放的话，**实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置**，这样一个数据块连着一个数据块，从链头开是就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。
![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677039-c71df217-651d-42ac-8cf6-444dedae8c1c.png)
隐式链表的存放方式的**缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间**。隐式链接分配的**稳定性较差**，系统在运行过程中由于软件或者硬件错误**导致链表中的指针丢失或损坏，会导致文件数据的丢失。**

如果取出每个磁盘块的指针，把它放在内存的一个表中，就可以解决上述隐式链表的两个不足。那么，这种实现方式是「**显式链接**」，它指**把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中**，该表在整个磁盘仅设置一张，**每个表项中存放链接指针，指向下一个数据块号**。

对于显式链接的工作方式，我们举个例子，文件 A 依次使用了磁盘块 4、7、2、10 和 12 ，文件 B 依次使用了磁盘块 6、3、11 和 14 。利用下图中的表，可以从第 4 块开始，顺着链走到最后，找到文件 A 的全部磁盘块。同样，从第 6 块开始，顺着链走到最后，也能够找出文件 B 的全部磁盘块。最后，这两个链都以一个不属于有效磁盘编号的特殊标记（如 -1 ）结束。内存中的这样一个表格称为**文件分配表（File Allocation Table，FAT）**。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677063-b7bb2a32-dde6-4c17-9119-096413902ae8.png)

由于查找记录的过程是在内存中进行的，因而不仅显著地**提高了检索速度**，而且**大大减少了访问磁盘的次数**。但也正是整个表都存放在内存中的关系，它的主要的缺点是**不适用于大磁盘\*\*。

比如，对于 200GB 的磁盘和 1KB 大小的块，这张表需要有 2 亿项，每一项对应于这 2 亿个磁盘块中的一个块，每项如果需要 4 个字节，那这张表要占用 800MB 内存，很显然 FAT 方案对于大磁盘而言不太合适。

> 接下来，我们来看看索引的方式。

链表的方式解决了连续分配的磁盘碎片和文件动态扩展的问题，但是不能有效支持直接访问（FAT 除外），索引的方式可以解决这个问题。
索引的实现是为每个文件创建一个「**索引数据块**」，里面存放的是**指向文件数据块的指针列表**，说白了就像书的目录一样，要找哪个章节的内容，看目录查就可以。

另外，**文件头需要包含指向「索引数据块」的指针**，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。
创建文件时，索引块的所有指针都设为空。当首次写入第 i 块时，先从空闲空间中取得一个块，再将其地址写到索引块的第 i 个条目。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677054-825db739-3c6c-4fec-bb21-1dd52effb539.png)

索引的方式优点在于：

- 文件的创建、增大、缩小很方便；
- 不会有碎片的问题；
- 支持顺序读写和随机读写；

由于索引数据也是存放在磁盘块的，如果文件很小，明明只需一块就可以存放的下，但还是需要额外分配一块来存放索引数据，所以缺陷之一就是存储索引带来的开销。

如果文件很大，大到一个索引数据块放不下索引信息，这时又要如何处理大文件的存放呢？我们可以通过组合的方式，来处理大文件的存。

先来看看链表 + 索引的组合，这种组合称为「**链式索引块**」，它的实现方式是**在索引数据块留出一个存放下一个索引数据块的指针**，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。那这种方式也会出现前面提到的链表方式的问题，万一某个指针损坏了，后面的数据也就会无法读取了。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677072-417dcaa4-348d-4a45-b9b0-aa170d0f3bd7.png)

还有另外一种组合方式是索引 + 索引的方式，这种组合称为「**多级索引块**」，实现方式是**通过一个索引块来存放多个索引数据块**，一层套一层索引，像极了俄罗斯套娃是吧。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677056-ecb3b996-c14c-455d-82aa-7ca16aeb7feb.png)

## Unix 文件的实现方式

我们先把前面提到的文件实现方式，做个比较：

![image.png](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1671074085764-374b9218-86e8-412d-85c7-9d051b9340b2.png)

那早期 Unix 文件系统是组合了前面的文件存放方式的优点，如下图：

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677095-e05fe455-c302-43bb-a9d8-dd43cd4551e1.png)

它是根据文件的大小，存放的方式会有所变化：

- 如果存放文件所需的数据块小于 10 块，则采用直接查找的方式；
- 如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式；
- 如果前面两种方式都不够存放大文件，则采用二级间接索引方式；
- 如果二级间接索引也不够存放大文件，这采用三级间接索引方式；

那么，文件头（_Inode_）就需要包含 13 个指针：

- 第 10 个指向数据块的指针；
- 第 11 个指向索引块的指针；
- 第 12 个指向二级索引块的指针；
- 第 13 个指向三级索引块的指针；

所以，这种方式能很灵活地支持小文件和大文件的存放：

- 对于小文件使用直接查找的方式可减少索引数据块的开销；
- 对于大文件则以多级索引的方式来支持，所以大文件在访问数据块时需要大量查询；

这个方案就用在了 Linux Ext 2/3 文件系统里，虽然解决大文件的存储，但是对于大文件的访问，需要大量的查询，效率比较低。

为了解决这个问题，Ext 4 做了一定的改变，具体怎么解决的，本文就不展开了。

# 空闲空间管理

前面说到的文件的存储是针对已经被占用的数据块组织和管理，接下来的问题是，如果我要保存一个数据块，我应该放在硬盘上的哪个位置呢？难道需要将所有的块扫描一遍，找个空的地方随便放吗？

那这种方式效率就太低了，所以针对磁盘的空闲空间也是要引入管理的机制，接下来介绍几种常见的方法：

- 空闲表法
- 空闲链表法
- 位图法

## 空闲表法

空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图：
![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677079-9e2989d6-2cda-4460-80ca-f3c0f28783f1.png)
当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。
这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。

## 空闲链表法

我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图：
![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677070-be98384a-ad4f-4a20-b5da-1d969816b938.png)
当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。

这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。

空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。

## 位图法

位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。

当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：

    1111110011111110001110110111111100111 ...

在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。

# 文件系统的结构

前面提到 Linux 是用位图的方式管理空闲空间，用户在创建一个新文件时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要存储数据时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的。

数据块的位图是放在磁盘块里的，假设是放在一个块里，一个块 4K，每位表示一个数据块，共可以表示 `4 * 1024 * 8 = 2^15` 个空闲块，由于 1 个数据块是 4K 大小，那么最大可以表示的空间为 `2^15 * 4 * 1024 = 2^27` 个 byte，也就是 128M。

也就是说按照上面的结构，如果采用「一个块的位图 + 一系列的块」，外加「一个块的 inode 的位图 + 一系列的 inode 的结构」能表示的最大空间也就 128M，这太少了，现在很多文件都比这个大。

在 Linux 文件系统，把这个结构称为一个**块组**，那么有 N 多的块组，就能够表示 N 大的文件。

下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布：

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/op2fw9/1616167677099-7ffa20c4-57d9-4c49-9e02-24afef066cfb.png)

最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下：

- **超级块** # 包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。
- **块组描述符** # 包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。
- **数据位图和 inode 位图** # 用于表示对应的数据块或 inode 是空闲的，还是被使用中。
- **inode 列表** # 包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。
- **数据块** # 包含文件的有用数据。

你可以会发现每个块组里有很多重复的信息，比如**超级块和块组描述符表，这两个都是全局信息，而且非常的重要**，这么做是有两个原因：

- 如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。
- 通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。

不过，Ext2 的后续版本采用了**稀疏技术**。该做法是，超级块和块组描述符表不再存储到文件系统的每个块组中，而是只写入到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中。

在创建文件系统时，也可以看到 `Superblock backups stored on blocks` 这种描述，这记录了超级块的备份存在哪些块中

```bash
~]# mke2fs -N 10000000 /dev/vdb
mke2fs 1.45.5 (07-Jan-2020)
/dev/vdb contains a ext2 file system
	created on Sat Mar 11 12:25:22 2023
Proceed anyway? (y,N) y
Creating filesystem with 9175040 4k blocks and 10002528 inodes
Filesystem UUID: 8acc177c-5f26-4bb9-a0ee-01ceb61d4eaa
Superblock backups stored on blocks: 
	30072, 90216, 150360, 210504, 270648, 751800, 811944, 1473528, 2435832, 
	3759000, 7307496

Allocating group tables: done                            
Writing inode tables: done                            
Writing superblocks and filesystem accounting information: done
```