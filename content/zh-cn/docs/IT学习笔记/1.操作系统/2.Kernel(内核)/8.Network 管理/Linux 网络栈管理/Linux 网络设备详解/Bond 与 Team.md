---
title: Bond 与 Team
---

# 概述

> 参考：
>
> - [Wiki,Link Aggregation](https://en.wikipedia.org/wiki/Link_aggregation)
> - [Wiki,MII](https://en.wikipedia.org/wiki/Media-independent_interface)
> - [Linux 内核文档,Linux 网络文档-Linux 以太网 Bonding 驱动入门指南](https://www.kernel.org/doc/html/latest/networking/bonding.html)(这里可以看到所有 Bonding 参数)
> - [Linux 基金会 Wiki,网络-bonding](https://wiki.linuxfoundation.org/networking/bonding)
> - 红帽官方的 bond 说明文档：<https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/networking_guide/ch-configure_network_bonding>
> - <https://www.ibm.com/docs/en/linux-on-systems?topic=recommendations-link-monitoring>

**Link Aggregation(链路聚合)** 技术就是将多条物理链路聚合成一条带宽更高的逻辑链路，该逻辑链路的带宽等于被聚合在一起的多条物理链路的带宽之和。聚合在一起的物理链路的条数可以根据业务的带宽需求来配置。因此链路聚合具有成本低，配置灵活的优点，此外，链路聚合还具有链路冗余备份的功能，聚合在一起的链路彼此动态备份，提高了网络的稳定性。早期链路聚合技术的实现没有统一的标准，各厂商都有自己私有的解决方案，功能不完全相同，也互不兼容。因此，IEEE 专门制定了链路聚合的标准，目前链路聚合技术的正式标准为 IEEE Standard 802.3ad，而 **Link Aggregation Control Protocol(链路汇聚控制协议,LACP)** 是该标准的主要内容之一，是一种实现链路动态聚合的协议。

## Link Aggregation Control Protocol

**Link Aggregation Control Protocol(链路汇聚控制协议，简称 LACP)** 在 IEEE 以太网标准中，提供了一种方法，可以将多个物理链路捆绑在一起以形成单个逻辑链路。LACP 允许网络设备通过将 LACP 数据包发送到 **Peer(对等方)** 以 **negotiate(协商)** 链路状态，并实现自动捆绑。

> Peer(对等方) 指的是与本网络设备直连的可以实现 LACP 的对端网络设备
> LACP 数据包通常称为 **Link Aggregation Control Protocol Data Unit(链路汇聚控制协议数据单元，简称 LACPDU)**

# Bond，网卡绑定

Bond 类型的网络设备是通过把多个网络设备绑定为一个逻辑网络设备，实现本地网络设备的冗余、带宽扩容和负载均衡。在应用部署中是一种常用的技术。

Linux 中使用 bonding 模块实现 bonding 驱动程序。

```bash
[lichenhao@hw-cloud-xngy-jump-server-linux-2 ~]$ modinfo bonding
filename:       /lib/modules/5.4.0-88-generic/kernel/drivers/net/bonding/bonding.ko
author:         Thomas Davis, tadavis@lbl.gov and many others
description:    Ethernet Channel Bonding Driver, v3.7.1
version:        3.7.1
license:        GPL
alias:          rtnl-link-bond
srcversion:     B95AF01257E8C745F584C8F
depends:
retpoline:      Y
intree:         Y
name:           bonding
vermagic:       5.4.0-88-generic SMP mod_unload modversions
sig_id:         PKCS#7
signer:         Build time autogenerated kernel key
sig_key:        2D:2D:71:A0:22:44:6D:60:C8:49:CB:0E:D7:43:D0:D2:7A:5C:0E:F1
sig_hashalgo:   sha512
signature:      AE:16:69:2D:17:C0:36:10:F4:52:73:EB:A4:CB:CB:FC:68:78:DE:3A:
......
parm:           max_bonds:Max number of bonded devices (int)
parm:           tx_queues:Max number of transmit queues (default = 16) (int)
parm:           num_grat_arp:Number of peer notifications to send on failover event (alias of num_unsol_na) (int)
parm:           num_unsol_na:Number of peer notifications to send on failover event (alias of num_grat_arp) (int)
parm:           miimon:Link check interval in milliseconds (int)
parm:           updelay:Delay before considering link up, in milliseconds (int)
parm:           downdelay:Delay before considering link down, in milliseconds (int)
parm:           use_carrier:Use netif_carrier_ok (vs MII ioctls) in miimon; 0 for off, 1 for on (default) (int)
parm:           mode:Mode of operation; 0 for balance-rr, 1 for active-backup, 2 for balance-xor, 3 for broadcast, 4 for 802.3ad, 5 for balance-tlb, 6 for balance-alb (charp)
parm:           primary:Primary network device to use (charp)
parm:           primary_reselect:Reselect primary slave once it comes up; 0 for always (default), 1 for only if speed of primary is better, 2 for only on active slave failure (charp)
parm:           lacp_rate:LACPDU tx rate to request from 802.3ad partner; 0 for slow, 1 for fast (charp)
parm:           ad_select:802.3ad aggregation selection logic; 0 for stable (default), 1 for bandwidth, 2 for count (charp)
parm:           min_links:Minimum number of available links before turning on carrier (int)
parm:           xmit_hash_policy:balance-alb, balance-tlb, balance-xor, 802.3ad hashing method; 0 for layer 2 (default), 1 for layer 3+4, 2 for layer 2+3, 3 for encap layer 2+3, 4 for encap layer 3+4 (charp)
parm:           arp_interval:arp interval in milliseconds (int)
parm:           arp_ip_target:arp targets in n.n.n.n form (array of charp)
parm:           arp_validate:validate src/dst of ARP probes; 0 for none (default), 1 for active, 2 for backup, 3 for all (charp)
parm:           arp_all_targets:fail on any/all arp targets timeout; 0 for any (default), 1 for all (charp)
parm:           fail_over_mac:For active-backup, do not set all slaves to the same MAC; 0 for none (default), 1 for active, 2 for follow (charp)
parm:           all_slaves_active:Keep all frames received on an interface by setting active flag for all slaves; 0 for never (default), 1 for always. (int)
parm:           resend_igmp:Number of IGMP membership reports to send on link failure (int)
parm:           packets_per_slave:Packets to send per slave in balance-rr mode; 0 for a random slave, 1 packet per slave (default), >1 packets per slave. (int)
parm:           lp_interval:The number of seconds between instances where the bonding driver sends learning packets to each slaves peer switch. The default is 1. (uint)
```

# Bond 参数

## ARP 监控参数

ARP 监控参数与 MII 监控参数不可同时使用

**arp_interval** # ARP 监控模式的监控频率，单位 毫秒。`默认值：0`。0 值表示禁用 ARP 监控
**arp_ip_target** #

## MII 监控参数

**Media Independent Interface(介质无关接口，简称 MII)**，通过该接口可以检测聚合链路的状态，当某个网络设备故障时，bonding 驱动会将这个故障设备标记为关闭。虽然不会将设备踢出聚合组，但是数据不在通过故障设备传输
MII 监控参数与 ARP 监控参数不可同时使用

**miimon** # MII 监控模式的监控频率，单位 毫秒。这决定了每个备链路状态的故障检查频率。`默认值：0`。0 值表示禁用 MII 链路监控

- 通常设置为 100

**use_carrier** # 指定 miimon 是否应使用 MII 或 ETHTOOL ioctls 与 netif_carrier_ok() 来确定链接状态。 默认值是 1，这使得可以使用 netif_carrier_ok（）。 这由 Linux on Z 上的 qeth 设备驱动程序支持。
**downdelay**# 检测到网络设备故障后，持续 downdelay 毫秒后，关闭该设备。
**updelay** # 检测到网络设备恢复后，持续 updelay 毫秒后，启用该设备

## Bond 模式参数

**mode** # 指定 bonding 策略。`默认值：balance-rr`。常见的 bond 模式有七种：括号中是该模式所对应的数字，使用 nmcli 命令时，不要使用数字代替，NetworkManager 无法识别数字。

- **balance-rr(0)** # 表示负载分担 round-robin，和交换机的聚合强制不协商的方式配合。
- **active-backup(1)** # 表示主备模式，只有一块网卡是 active,另外一块是备的 standby，这时如果交换机配的是捆绑，将不能正常工作，因为交换机往两块网卡发包，有一半包是丢弃的。
  - 注意：vmwork 的虚拟机中只能做 mode=1 的实验，其它的工作模式得用真机来实践，并且需要添加额外参数(fail_over_mac=1)才能实现主备模式
- **balance-xor(2)** # 表示 XOR Hash 负载分担，和交换机的聚合强制不协商方式配合(需要 xmit_hash_policy)
  - 推荐 bond 参数：mode=balance-xor,miimon=100,xmit_hash_policy=layer3+4
- **broadcast(3)** # 表示所有包从所有 interface 发出，这个不均衡，只有冗余机制...和交换机的聚合强制不协商方式配合。
- **802.3ad(4)** # 表示支持 802.3ad 协议，动态链路聚合，需要和交换机的聚合 LACP 方式配合(需要 xmit_hash_policy)
  - 推荐 bond 参数：mode=802.3ad,miimon=100,lacp_rate=1,xmit_hash_policy=layer3+4
  - 802.3ad 模式的 Bond 网络设备的最大带宽是所有 Slave 设备最大带宽之和
- **balance-tlb(5)** # 是根据每个 slave 的负载情况选择 slave 进行发送，接收时使用当前轮到的 slave
- **balance-alb(6)** # 在 5 的 tlb 基础上增加了 rlb。Adaptive Load Balancing(简称 ALB) 协议，可以根据网络状态和物理网卡的带宽来动态地将数据包分配到每个物理网卡上。

> 注意：
>
> - active-backup、balance-tlb 和 balance-alb 模式不需要交换机的任何特殊配置。其他绑定模式需要配置交换机以便整合链接。例如：Cisco 交换机需要在模式 0、2 和 3 中使用 EtherChannel，但在模式 4 中需要 LACP 和 EtherChannel。有关交换机附带文档，请查看 <https://www.kernel.org/doc/Documentation/networking/bonding.txt>。
> - 若想 bond 功能生效，关闭 NetworkManager 服务 或者 在配置文件中的 BONDING_OPTS 中 MODE 的值不要用数字，而是直接使用模式名称来作为值

每种模式的理论最大带宽：

- 0 和 2 模式是所有网络设备的带宽之和
- 1 和 3 模式是单个网络设备的带宽
- 4 模式也是带宽之和，但是会动态分配，有时候最大值可能会超过带宽之和的上限
- 5 和 6 与 4 类似，都是带宽之和，且可能超过上限。

## 其他参数

**lacp_rate** # 作用于 802.3ad 模式。向聚合链路的对端(通常来说都是交换机)传输**LACPDU** 包的速率。可用的值有如下几个：

- slow # 每 30 秒传输一次 LACPDU，即每 30 秒协商一次
- fast # 每秒传输一次 LACPDU，即每秒协商一次
- max_bonds # 指定要为此绑定驱动程序实例创建的绑定设备数。例如，如果 max_bonds 为 3，并且绑定驱动程序尚未加载，则将创建 bond0、bond1 和 bond2。默认值为 1

**xmit_hash_policy** # 作用于 balance-xor 和 802.3ad 模式。配置传输 hash 策略。`默认值：layer2`。

- layer2 # 该策略支持 802.3ad。使用 XOR 或硬件 MAC 地址生成 hash。
- layer2+3 # 该策略支持 802.3ad。使用 XOR 或硬件 MAC 地址与 IP 地址一起生成 hash。
- layer3+4 # 该策略不完全支持 802.3ad
- 说明：
  - 这里面的 2，3，4 其实就是指的 ISO 模型里的层，2 层就是用 MAC 进行计算，2+3 就是用 MAC 加 IP 进行计算，3+4 就是用 IP 加 PORT 进行计算。
  - 只使用 2 层的 MAC 进行计算时，会导致同一个网关的数据流将完全从一个端口发送，但是如果使用 2+3 或 3+4，虽然负载更均衡了，但是由于使用了上层协议进行计算，则增加了 hash 的开销。
  - 计算越负责，负载均衡效果越好，但是资源开销越大。

# Bond 配置

/sys/class/net/bonding_masters # 当前系统下已经启用的 Bond 名称
/sys/class/net/BondName/\* # Bond 类型网络设备的运行时信息，这里面某些信息是可以被修改的。

- ./statistics/\* # 网络设备的状态信息，比如 发送/接受 了多少数据包、多少数据量 等等，`ip -s` 参数可以从这里获取到信息
- ./bonding/\* # Bond 参数

/proc/net/bonding/\* # bond 运行时状态信息。其内文件名为 Bond 的名称。查看文件内容解析详见[《Bonding 在内核中的信息解析》](#cvrU1)部分

## Bond 基本配置文件详解

首先是配置一个 bond 网络设备，IP 等相关信息配置在 bond 网络设备上。

```bash
[root@lichenhao network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-bond0
BONDING_OPTS="mode=balance-rr"
TYPE=Bond
BONDING_MASTER=yes
BOOTPROTO=dhcp
DEFROUTE=yes
NAME=bond0
DEVICE=bond0
ONBOOT=yes
```

其次配置让一个物理网络设备绑定到该 bond 设备上

```bash
[root@lichenhao network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-bond-slave-em1
TYPE=Ethernet
NAME=bond-slave-em1
DEVICE=em1
ONBOOT=yes
MASTER=bond0
SLAVE=yes
```

# Bond 在内核中的信息解析

/proc/net/bonding/BondNAME 文件中保存了当前系统中已启动的 Bond 类型的网络设备的信息
这些信息分为多个部分

- Bond 类型的网络设备通用信息
- Bond 类型的网络设备特定信息，比如 802.3ad 类型的 Bond 就有独自的信息
- Bond 下 Slave 网络设备通用信息

## balance-rr

```bash
Bonding Mode: load balancing (round-robin)
MII Status: up
MII Polling Interval (ms): 100
Up Delay (ms): 0
Down Delay (ms): 0

Slave Interface: enp6s0f0
MII Status: up
Speed: 10000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 40:a6:b7:25:f2:3c
Slave queue ID: 0

Slave Interface: enp6s0f1
MII Status: up
Speed: 10000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 40:a6:b7:25:f2:3d
Slave queue ID: 0
```

**Bond 设备**

- **Bonding Mode** # Bond 模式的名称
- **MII Status** # 链路监控状态
- **MII Polling Interval** #
- **Up Delay** # 检测到 Slave 网络设备恢复后，持续 updelay 毫秒后，启用 Slave 设备
- **Down Delay** # 检测到 Slave 网络设备故障后，持续 downdelay 毫秒后，关闭 Slave 设备。

**Slave 设备**

- **Slave Interface** # 与 Bond 设备关联的 Slave 网络设备的名称
- **MII Status** # 链路监控状态
- **Speed**# 最大传输速度，即网卡的带宽
- **Duplex** # 双工模式
- **Link Failure Count** # 失联总次数
- **Permanent HW addr** # 关联的物理网卡的硬件地址，即网卡的 MAC 地址
- **Slave queue ID** # 队列 ID

## balance-xor

```bash
[root@vs-7 bonding]# cat /proc/net/bonding/bond0
Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)

Bonding Mode: load balancing (xor) # 此 bond 的模式
Transmit Hash Policy: layer3+4 (1) # 此 bond 模式的参数
MII Status: up
MII Polling Interval (ms): 100
Up Delay (ms): 0
Down Delay (ms): 0
Peer Notification Delay (ms): 0

Slave Interface: eno1
MII Status: up
Speed: 1000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: f0:d4:e2:ea:28:54
Slave queue ID: 0

Slave Interface: eno2
MII Status: up
Speed: 1000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: f0:d4:e2:ea:28:55
Slave queue ID: 0
```

## 802.3ad

> - [StackOverflow,bonding LACP 模式下 Churn 的含义](https://stackoverflow.com/questions/62173444/churn-state-meaning-in-lacp-bonding)
> - <https://mlog.club/article/2693580>
>   - <https://bugzilla.redhat.com/show_bug.cgi?id=1295423>
>   - <https://git.kernel.org/pub/scm/linux/kernel/git/netdev/net.git/commit/?id=ea53abfab960909d622ca37bcfb8e1c5378d21cc>
>   - <https://access.redhat.com/solutions/4122011>
> - <https://github.com/systemd/systemd/issues/15208>

```bash
~]# cat /proc/net/bonding/bond1
Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)

Bonding Mode: IEEE 802.3ad Dynamic link aggregation
Transmit Hash Policy: layer3+4 (1)
MII Status: up
MII Polling Interval (ms): 100
Up Delay (ms): 0
Down Delay (ms): 0

802.3ad info
LACP rate: slow
Min links: 0
Aggregator selection policy (ad_select): stable
System priority: 65535
System MAC address: 32:1c:0d:e9:ca:e9
Active Aggregator Info:
    Aggregator ID: 1
    Number of ports: 2
    Actor Key: 15
    Partner Key: 6
    Partner Mac Address: 00:00:00:00:00:06

Slave Interface: enp6s0f0
MII Status: up
Speed: 10000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 40:a6:b7:26:60:b4
Slave queue ID: 0
Aggregator ID: 1
Actor Churn State: none
Partner Churn State: none
Actor Churned Count: 0
Partner Churned Count: 0
details actor lacp pdu:
    system priority: 65535
    system mac address: 32:1c:0d:e9:ca:e9
    port key: 15
    port priority: 255
    port number: 1
    port state: 61
details partner lacp pdu:
    system priority: 6
    system mac address: 00:00:00:00:00:06
    oper key: 6
    port priority: 32768
    port number: 33862
    port state: 63

Slave Interface: enp6s0f1
MII Status: up
Speed: 10000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 40:a6:b7:26:60:b5
Slave queue ID: 0
Aggregator ID: 1
Actor Churn State: none
Partner Churn State: none
Actor Churned Count: 0
Partner Churned Count: 0
details actor lacp pdu:
    system priority: 65535
    system mac address: 32:1c:0d:e9:ca:e9
    port key: 15
    port priority: 255
    port number: 2
    port state: 61
details partner lacp pdu:
    system priority: 6
    system mac address: 00:00:00:00:00:06
    oper key: 6
    port priority: 32768
    port number: 33862
    port state: 63
```

802.3ad 模式的 Bond 需要与交换机交互 LACP 信息，这里对 服务器 和 交换机 的称呼如下：

- **Actor** # 指主机，即服务器
- **Partner** # 指交换机

**Aggregator ID** #
**Actor Churn State 与 Partner Churn State** # 务器与交换机的 Churn 状态

- Churn State # Churn 状态共有三种，先是 monitoring，然后是 churned，最后 none 就正常了。
  - monitoring # 等待其他 PDUs 达成共识
    - The bond slave interface is in the process of the initial LACP communication with the LACP peer.
  - churned # 扰动、流逝
    - One of the peer's LACP (etherchannel) interfaces is suspended or is otherwise no longer active as an LACP interface
  - none # 链路已同步

**Actor Churned Count 与 Partner Churned Count** #
**details actor lacp pdu 与 details partner lacp pdu** # 服务器与交换机的 LACPDU 信息细节

![image.png](https://notes-learning.oss-cn-beijing.aliyuncs.com/ggzysy/1644663197947-87aaf3c3-d762-4682-8747-059014f1a7bc.png)

# Team，类似于 Bond，比 Bond 更优秀

# RHEL7 中网卡绑定 team 和 bond 的区别

red hat 官方给出的 team 和 bond 特性对比

A Comparison of Features in Bonding and Team

|                                                              |                   |               |
| ------------------------------------------------------------ | ----------------- | ------------- |
| Feature                                                      | Bonding           | Team          |
| broadcast Tx policy                                          | Yes               | Yes           |
| round-robin Tx policy                                        | Yes               | Yes           |
| active-backup Tx policy                                      | Yes               | Yes           |
| LACP (802.3ad) support                                       | Yes (active only) | Yes           |
| Hash-based Tx policy                                         | Yes               | Yes           |
| User can set hash function                                   | No                | Yes           |
| Tx load-balancing support (TLB)                              | Yes               | Yes           |
| LACP hash port select                                        | Yes               | Yes           |
| load-balancing for LACP support                              | No                | Yes           |
| Ethtool link monitoring                                      | Yes               | Yes           |
| ARP link monitoring                                          | Yes               | Yes           |
| NS/NA (IPv6) link monitoring                                 | No                | Yes           |
| ports up/down delays                                         | Yes               | Yes           |
| port priorities and stickiness (“primary”option enhancement) | No                | Yes           |
| separate per-port link monitoring setup                      | No                | Yes           |
| multiple link monitoring setup                               | Limited           | Yes           |
| lockless Tx/Rx path                                          | No (rwlock)       | Yes (RCU)     |
| VLAN support                                                 | Yes               | Yes           |
| user-space runtime control                                   | Limited           | Full          |
| Logic in user-space                                          | No                | Yes           |
| Extensibility                                                | Hard              | Easy          |
| Modular design                                               | No                | Yes           |
| Performance overhead                                         | Low               | Very Low      |
| D-Bus interface                                              | No                | Yes           |
| multiple device stacking                                     | Yes               | Yes           |
| zero config using LLDP                                       | No                | (in planning) |
| NetworkManager support                                       | Yes               | Yes           |
