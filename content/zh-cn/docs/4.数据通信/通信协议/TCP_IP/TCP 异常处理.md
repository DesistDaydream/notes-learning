---
title: TCP 异常处理
linkTitle: TCP 异常处理
date: 2024-09-18T17:08
weight: 20
---

# 概述

> 参考：
>
> -

有些 TCP 异常处理的详解在 WireShark 目录下的 [TCP Analysis](/docs/7.信息安全/Packet%20analyzer/WireShark/TCP%20Analysis.md) 笔记中

# 万字长文 | 23 个问题 TCP 疑难杂症全解析

原文: https://juejin.cn/post/6869734247465402382

- https://mp.weixin.qq.com/s/cyeeXx7fLZ_ngB-QnMM_TQ

> 每个时代，都不会亏待会学习的人。

在进入今天主题之前我先抛几个问题，这篇文章一共提出 23 个问题。

> TCP 握手一定是三次？TCP 挥手一定是四次？
> 
> 为什么要有快速重传，超时重传不够用？为什么要有 SACK，为什么要有 D-SACK？
> 
> 都知道有滑动窗口，那由于接收方的太忙了滑动窗口降为了 0 怎么办？发送方就永远等着了？
> 
> Silly Window 又是什么？
> 
> 为什么有滑动窗口流控还需要拥塞控制？
> 
> 快速重传一定要依赖三次重复 ACK ？

这篇文章我想**由浅到深地过一遍 TCP，不是生硬的搬出各个知识点，从问题入手，然后从发展、演进的角度来看 TCP**。

起初我在学计算机网络的时候就有非常非常多的疑问，脑子里简直充满了十万个为什么，而网络又非常的复杂，发展了这么多年东西真的太多了，今天我就**大致的浅显地说一说我对 TCP 这些要点的理解**。

好了，废话不多说，开始上正菜。

TCP 是用来解决什么问题？
--------------

TCP 即 Transmission Control Protocol，可以看到是一个传输控制协议，重点就在这个**控制**。

**控制什么？**

控制可靠、按序地传输以及端与端之间的流量控制。够了么？还不够，它需要更加智能，因此还需要加个拥塞控制，需要为整体网络的情况考虑。

这就是**出行你我他，安全靠大家**。

为什么要 TCP，IP 层实现控制不行么？
---------------------

我们知道网络是分层实现的，网络协议的设计就是为了通信，从链路层到 IP 层其实就已经可以完成通信了。

你看链路层不可或缺毕竟咱们电脑都是通过链路相互连接的，然后 IP 充当了地址的功能，所以通过 IP 咱们找到了对方就可以进行通信了。

那加个 TCP 层干啥？IP 层实现控制不就完事了嘛？

之所以要**提取出一个 TCP 层来实现控制是因为 IP 层涉及到的设备更多**，一条数据在网络上传输需要经过很多设备，而设备之间需要靠 IP 来寻址。

假设 IP 层实现了控制，那是不是涉及到的设备都需要关心很多事情？整体传输的效率是不是大打折扣了？

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f3101987aaa243839ee5800be2f82188~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

我举个例子，假如 A 要传输给 F 一个积木，但是无法直接传输到，需要经过 B、C、D、E 这几个中转站之手。 这里有两种情况：

*   假设 BCDE 都需要关心这个积木搭错了没，都拆开包裹仔细的看看，没问题了再装回去，最终到了 F 的手中。
*   假设 BCDE 都不关心积木的情况，来啥包裹只管转发就完事了，由最终的 F 自己来检查这个积木答错了没。

你觉得哪种效率高？明显是第二种，转发的设备不需要关心这些事，只管转发就完事！

所以把控制的逻辑独立出来成 TCP 层，让真正的接收端来处理，这样网络整体的传输效率就高了。

连接到底是什么？
--------

我们已经知道了为什么需要独立出 TCP 这一层，并且这一层主要是用来干嘛的，接下来就来看看它到底是怎么干的。

我们都知道 TCP 是面向连接的，那这个连接到底是个什么东西？真的是拉了一条线让端与端之间连起来了？

**所谓的连接其实只是双方都维护了一个状态，通过每一次通信来维护状态的变更**，使得看起来好像有一条线关联了对方。

TCP 协议头
-------

在具体深入之前我们需要先来看看一些 TCP 头的格式，这很基础也很重要。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/74a0bcb840724fffb35baafdded0c2dd~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

我就不一一解释了，挑重点的说。

首先可以看到 TCP 包只有端口，没有 IP。

Seq 就是 Sequence Number 即序号，它是用来解决乱序问题的。

ACK 就是 Acknowledgement Numer 即确认号，它是用来解决丢包情况的，告诉发送方这个包我收到啦。

标志位就是 TCP flags 用来标记这个包是什么类型的，用来控制 TPC 的状态。

窗口就是滑动窗口，Sliding Window，用来流控。

三次握手
----

明确了协议头的要点之后，我们再来看三次握手。

三次握手真是个老生常谈的问题了，但是真的懂了么？不是浮在表面？能不能延伸出一些点别的？

我们先来看一下熟悉的流程。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b227156ef199451eacae4eeba587a225~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

首先为什么要握手，**其实主要就是为了初始化 Seq Numer**，SYN 的全称是 Synchronize Sequence Numbers，这个序号是用来保证之后传输数据的顺序性。

你要说是为了测试保证双方发送接收功能都正常，我觉得也没毛病，**不过我认为重点在于同步序号**。

那为什么要三次，就拿我和你这两个角色来说，首先我告诉你我的初始化序号，你听到了和我说你收到了。

然后你告诉我你的初始序号，然后我对你说我收到了。

这好像四次了？如果真的按一来一回就是四次，但是中间一步可以合在一起，就是你和我说你知道了我的初始序号的时候同时将你的初始序号告诉我。

因此四次握手就可以减到三次了。

不过你没有想过这么一种情形，我和你同时开口，一起告诉对方各自的初始序号，然后分别回应收到了，这不就是四次握手了？

我来画个图，清晰一点。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c1a16c4c4c534a48957b3a8141b0b851~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

看看是不是四次握手了? 不过具体还是得看实现，有些实现可能不允许这种情况出现，但是这不影响我们思考，因为**握手的重点就是同步初始序列号**，这种情况也完成了同步的目标。

初始序列号 ISN 的取值
-------------

不知道大家有没有想过 ISN 的值要设成什么？代码写死从零开始？

想象一下如果写死一个值，比如 0 ，那么假设已经建立好连接了，client 也发了很多包比如已经第 20 个包了，然后网络断了之后 client 重新，端口号还是之前那个，然后序列号又从 0 开始，此时服务端返回第 20 个包的ack，客户端是不是傻了？

所以 RFC793 中认为 ISN 要和一个假的时钟绑定在一起 **ISN 每四微秒加一，当超过 2 的 32 次方之后又从 0 开始，要四个半小时左右发生 ISN 回绕**。

所以 ISN 变成一个递增值，真实的实现还需要加一些随机值在里面，防止被不法份子猜到 ISN。

SYN 超时了怎么处理？
------------

也就是 client 发送 SYN 至 server 然后就挂了，此时 server 发送 SYN+ACK 就一直得不到回复，怎么办？

我脑海中一想到的就是重试，但是不能连续快速重试多次，你想一下，假设 client 掉线了，你总得给它点时间恢复吧，所以呢需要**慢慢重试，阶梯性重试**。

在 Linux 中就是默认重试 5 次，并且就是阶梯性的重试，间隔就是1s、2s、4s、8s、16s，再第五次发出之后还得等 32s 才能知道这次重试的结果，所以说总共等63s 才能断开连接。

SYN Flood 攻击
------------

你看到没 SYN 超时需要耗费服务端 63s 的时间断开连接，也就说 63s 内服务端需要保持这个资源，所以不法分子就可以构造出大量的 client 向 server 发 SYN 但就是不回 server。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f7687c49e83e4852b5f4028d4ccac741~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

使得 server 的 SYN 队列耗尽，无法处理正常的建连请求。

所以怎么办？

可以开启 tcp\_syncookies，那就用不到 SYN 队列了。

SYN 队列满了之后 TCP 根据自己的 ip、端口、然后对方的 ip、端口，对方 SYN 的序号，时间戳等一波操作生成一个特殊的序号（即 cookie）发回去，如果对方是正常的 client 会把这个序号发回来，然后 server 根据这个序号建连。

或者调整 tcp\_synack\_retries 减少重试的次数，设置 tcp\_max\_syn\_backlog 增加 SYN 队列数，设置 tcp\_abort\_on\_overflow SYN 队列满了直接拒绝连接。

为什么要四次挥手？
---------

四次挥手和三次握手成双成对，同样也是 TCP 中的一线明星，让我们重温一下熟悉的图。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/830dfa93d4114700bec3dac7efe36ca4~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

为什么挥手需要四次？**因为 TCP 是全双工协议**，也就是说双方都要关闭，每一方都向对方发送 FIN 和回应 ACK。

就像我对你说我数据发完了，然后你回复好的你收到了。然后你对我说你数据发完了，然后我向你回复我收到了。

所以看起来就是四次。

从图中可以看到主动关闭方的状态是 FIN\_WAIT\_1 到 FIN\_WAIT\_2 然后再到 TIME\_WAIT，而被动关闭方是 CLOSE\_WAIT 到 LAST\_ACK。

四次挥手状态一定是这样变迁的吗
---------------

状态一定是这样变迁的吗？让我们再来看个图。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6f45260c9bbd42ef8770ed57af40d137~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

可以看到双方都主动发起断开请求所以各自都是主动发起方，状态会从 FIN\_WAIT\_1 都进入到 CLOSING 这个过度状态然后再到 TIME\_WAIT。

挥手一定需要四次吗？
----------

假设 client 已经没有数据发送给 server 了，所以它发送 FIN 给 server 表明自己数据发完了，不再发了，如果这时候 server 还是有数据要发送给 client 那么它就是先回复 ack ，然后继续发送数据。

等 server 数据发送完了之后再向 client 发送 FIN 表明它也发完了，然后等 client 的 ACK 这种情况下就会有四次挥手。

那么假设 client 发送 FIN 给 server 的时候 server 也没数据给 client，那么 server 就可以将 ACK 和它的 FIN 一起发给client ，然后等待 client 的 ACK，这样不就三次挥手了？

为什么要有 TIME\_WAIT?
-----------------

断开连接发起方在接受到接受方的 FIN 并回复 ACK 之后并没有直接进入 CLOSED 状态，而是进行了一波等待，等待时间为 2MSL。

MSL 是 Maximum Segment Lifetime，即报文最长生存时间，RFC 793 定义的 MSL 时间是 2 分钟，Linux 实际实现是 30s，那么 2MSL 是一分钟。

**那么为什么要等 2MSL 呢？**

*   就是怕被动关闭方没有收到最后的 ACK，如果被动方由于网络原因没有到，那么它会再次发送 FIN， 此时如果主动关闭方已经 CLOSED 那就傻了，因此等一会儿。
    
*   假设立马断开连接，但是又重用了这个连接，就是五元组完全一致，并且序号还在合适的范围内，虽然概率很低但理论上也有可能，那么新的连接会被已关闭连接链路上的一些残留数据干扰，因此给予一定的时间来处理一些残留数据。
    

等待 2MSL 会产生什么问题？
----------------

如果服务器主动关闭大量的连接，那么会出现大量的资源占用，需要等到 2MSL 才会释放资源。

如果是客户端主动关闭大量的连接，那么在 2MSL 里面那些端口都是被占用的，端口只有 65535 个，如果端口耗尽了就无法发起送的连接了，不过我觉得这个概率很低，这么多端口你这是要建立多少个连接？

如何解决 2MSL 产生的问题？
----------------

**快速回收**，即不等 2MSL 就回收， Linux 的参数是 tcp\_tw\_recycle，还有 tcp\_timestamps 不过默认是打开的。

其实上面我们已经分析过为什么需要等 2MSL，所以如果等待时间果断就是出现上面说的那些问题。

所以不建议开启，而且 Linux 4.12 版本后已经咔擦了这个参数了。

前不久刚有位朋友在群里就提到了这玩意。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/74fa51010eb844eeb1b4e2be9692fc5a~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

一问果然有 NAT 的身影。

现象就是请求端请求服务器的静态资源偶尔会出现 20-60 秒左右才会有响应的情况，从抓包看请求端连续三个 SYN 都没有回应。

比如你在学校，对外可能就一个公网 IP，然后开启了 tcp\_tw\_recycle（tcp\_timestamps 也是打开的情况下），在 60 秒内对于同源 IP 的连接请求中 timestamp 必须是递增的，不然认为其是过期的数据包就会丢弃。

学校这么多机器，你无法保证时间戳是一致的，因此就会出问题。

所以这玩意不推荐使用。

**重用**，即开启 tcp\_tw\_reuse 当然也是需要 tcp\_timestamps 的。

这里有个重点，**tcp\_tw\_reuse 是用在连接发起方的，而我们的服务端基本上是连接被动接收方**。

tcp\_tw\_reuse 是发起新连接的时候，可以复用超过 1s 的处于 TIME\_WAIT 状态的连接，所以它压根没有减少我们服务端的压力。

**它重用的是发起方处于 TIME\_WAIT 的连接**。

这里还有一个 SO\_REUSEADDR ，这玩意有人会和 tcp\_tw\_reuse 混为一谈，首先 tcp\_tw\_reuse 是内核选项而 SO\_REUSEADDR 是用户态选项。

然后 SO\_REUSEADDR 主要用在你启动服务的时候，如果此时的端口被占用了并且这个连接处于 TIME\_WAIT 状态，那么你可以重用这个端口，如果不是 TIME\_WAIT，那就是给你个 Address already in use。

所以这两个玩意好像都不行，而且 tcp\_tw\_reuse 和 tcp\_tw\_recycle，其实是违反 TCP 协议的，说好的等我到天荒地老，你却偷偷放了手？

要么就是调小 MSL 的时间，不过也不太安全，要么调整 tcp\_max\_tw\_buckets 控制 TIME\_WAIT 的数量，不过默认值已经很大了 180000，这玩意应该是用来对抗 DDos 攻击的。

所以我给出的建议是**服务端不要主动关闭，把主动关闭方放到客户端**。毕竟咱们服务器是一对很多很多服务，我们的资源比较宝贵。

自己攻击自己
------

还有一个很骚的解决方案，我自己瞎想的，就是自己攻击自己。

Socket 有一个选项叫 IP\_TRANSPARENT ，可以绑定一个非本地的地址，然后服务端把建连的 ip 和端口都记下来，比如写入本地某个地方。

然后启动一个服务，假如现在服务端资源很紧俏，那么你就定个时间，过了多久之后就将处于 TIME\_WAIT 状态的对方 ip 和端口告诉这个服务。

然后这个服务就利用 IP\_TRANSPARENT 伪装成之前的那个 client 向服务端发起一个请求，然后服务端收到会给真的 client 一个 ACK， 那 client 都关了已经，说你在搞啥子，于是回了一个 RST，然后服务端就中止了这个连接。

超时重传机制是为了解决什么问题？
----------------

前面我们提到 TCP 要提供可靠的传输，那么网络又是不稳定的如果传输的包对方没收到却又得保证可靠那么就必须重传。

TCP 的可靠性是靠确认号的，比如我发给你1、2、3、4这4个包，你告诉我你现在要 5 那说明前面四个包你都收到了，就是这么回事儿。

不过这里要注意，SeqNum 和 ACK 都是**以字节数为单位的**，也就是说假设你收到了1、2、4 但是 3 没有收到你不能 ACK 5，如果你回了 5 那么发送方就以为你5之前的都收到了。

所以**只能回复确认最大连续收到包**，也就是 3。

而发送方不清楚 3、4 这两个包到底是还没到呢还是已经丢了，于是发送方需要等待，这等待的时间就比较讲究了。

如果太心急可能 ACK 已经在路上了，你这重传就是浪费资源了，如果太散漫，那么接收方急死了，这死鬼怎么还不发包来，我等的花儿都谢了。

所以这个等待超时重传的时间很关键，怎么搞？聪明的小伙伴可能一下就想到了，你估摸着正常来回一趟时间是多少不就好了，我就等这么长。

这就来回一趟的时间就叫 RTT，即 Round Trip Time，然后根据这个时间制定超时重传的时间 RTO，即 Retransmission Timeout。

不过这里大概只好了 RTO 要参考下 RTT ，但是具体要怎么算？首先肯定是采样，然后一波加权平均得到 RTO。

RFC793 定义的公式如下：

> 1、先采样 RTT 2、SRTT = ( ALPHA \* SRTT ) + ((1-ALPHA) \* RTT) 3、RTO = min\[UBOUND,max\[LBOUND,(BETA\*SRTT)\]\]

ALPHA 是一个平滑因子取值在 0.8~0.9之间，UBOUND 就是超时时间上界-1分钟，LBOUND 是下界-1秒钟，BETA 是一个延迟方差因子，取值在 1.3~2.0。

但是还有个问题，RTT 采样的时间用一开始发送数据的时间到收到 ACK 的时间作为样本值还是重传的时间到 ACK 的时间作为样本值？

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5f83462edad5416d8f40db7a7f823fc6~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

从图中就可以看到，一个时间算长了，一个时间算短了，这有点难，因为你不知道这个 ACK 到底是回复谁的。

所以怎么办？**发生重传的来回我不采样不就好了**，我不知道这次 ACK 到底是回复谁的，我就不管他，我就采样正常的来回。

这就是 Karn / Partridge 算法，不采样重传的RTT。

但是不采样重传会有问题，比如某一时刻网络突然就是很差，你要是不管重传，那么还是按照正常的 RTT 来算 RTO， 那么超时的时间就过短了，于是在网络很差的情况下还疯狂重传加重了网络的负载。

因此 Karn 算法就很粗暴的搞了个发生重传我就将现在的 RTO 翻倍。

但是这种平均的计算很容易把一个突然间的大波动，平滑掉，所以又搞了个算法，叫 Jacobson/Karels Algorithm。

它把最新的 RTT 和平滑过的 SRTT 做了波计算得到合适的 RTO，公式我就不贴了，反正我不懂，不懂就不哔哔了。

为什么还需要快速重传机制？
-------------

超时重传是按时间来驱动的，如果是网络状况真的不好的情况，超时重传没问题，但是如果网络状况好的时候，只是恰巧丢包了，那等这么长时间就没必要。

于是又引入了数据驱动的重传叫快速重传，什么意思呢？就是发送方如果连续三次收到对方相同的确认号，那么马上重传数据。

因为连续收到三次相同 ACK 证明当前网络状况是 ok 的，那么确认是丢包了，于是立马重发，没必要等这么久。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ad9b821127484b138e9c94b6972c065d~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

看起来好像挺完美的，但是你有没有想过我发送1、2、3、4这4个包，就 2 对方没收到，1、3、4都收到了，然后不管是超时重传还是快速重传反正对方就回 ACK 2。

这时候要重传 2、3、4 呢还是就 2 呢？

SACK 的引入是为了解决什么问题？
------------------

SACK 即 Selective Acknowledgment，它的引入就是为了解决发送方不知道该重传哪些数据的问题。

我们来看一下下面的图就知道了。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a2063b8e252544d892cd0b58eb65d46f~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

SACK 就是接收方会回传它已经接受到的数据，这样发送方就知道哪一些数据对方已经收到了，所以就可以选择性的发送丢失的数据。

如图，通过 ACK 告知我接下来要 5500 开始的数据，并一直更新 SACK，6000-6500 我收到了，6000-7000的数据我收到了，6000-7500的数据我收到了，发送方很明确的知道，5500-5999 的那一波数据应该是丢了，于是重传。

而且如果数据是多段不连续的， SACK 也可以发送，比如 SACK 0-500,1000-1500，2000-2500。就表明这几段已经收到了。

D-SACK 又是什么东西？
--------------

D-SACK 其实是 SACK 的扩展，它利用 SACK 的第一段来描述重复接受的不连续的数据序号，如果第一段描述的范围被 ACK 覆盖，说明重复了，比如我都 ACK 到6000了你还给我回 SACK 5000-5500 呢？

说白了就是从第一段的反馈来和已经接受到的 ACK 比一比，参数是 tcp\_dsack，Linux 2.4 之后默认开启。

那知道重复了有什么用呢？

1、知道重复了说明对方收到刚才那个包了，所以是回来的 ACK 包丢了。 2、是不是包乱序的，先发的包后到？ 3、是不是自己太着急了，RTO 太小了？ 4、是不是被数据复制了，抢先一步呢？

滑动窗口干嘛用？
--------

我们已经知道了 TCP 有序号，并且还有重传，但是这还不够，因为我们不是愣头青，还需要根据情况来控制一下发送速率，因为网络是复杂多变的，有时候就会阻塞住，而有时候又很通畅。

所以发送方需要知道接收方的情况，好控制一下发送的速率，不至于蒙着头一个劲儿的发然后接受方都接受不过来。

因此 TCP 就有个叫滑动窗口的东西来做流量控制，也就是接收方告诉发送方我还能接受多少数据，然后发送方就可以根据这个信息来进行数据的发送。

以下是**发送方维护的窗口**，就是黑色圈起来的。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/787e1e0eef894660b8fc3f60fe1cdb47~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

图中的 #1 是已收到 ACK 的数据，#2 是已经发出去但是还没收到 ACK 的数据，#3 就是在窗口内可以发送但是还没发送的数据。#4 就是还不能发送的数据。

然后此时收到了 36 的 ACK，并且发出了 46-51 的字节，于是窗口向右滑动了。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d56f7cd962db4d7f8fe95c24621124b9~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

TCP/IP Guide 上还有一张完整的图，画的十分清晰，大家看一下。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/debf6c7632614b85bc20226e7e82739c~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

如果接收方回复的窗口一直是 0 怎么办？
--------------------

上文已经说了发送方式根据接收方回应的 window 来控制能发多少数据，如果接收方一直回应 0，那发送方就杵着？

你想一下，发送方发的数据都得到 ACK 了，但是呢回应的窗口都是 0 ，这发送方此时不敢发了啊，那也不能一直等着啊，这 Window 啥时候不变 0 啊？

于是 TCP 有一个 Zero Window Probe 技术，发送方得知窗口是 0 之后，会去探测探测这个接收方到底行不行，也就是发送 ZWP 包给接收方。

具体看实现了，可以发送多次，然后还有间隔时间，多次之后都不行可以直接 RST。

假设接收方每次回应窗口都很小怎么办？
------------------

你想象一下，如果每次接收方都说我还能收 1 个字节，发送方该不该发？

TCP + IP 头部就 40 个字节了，这传输不划算啊，如果傻傻的一直发这就叫 Silly Window。

那咋办，一想就是发送端等着，等养肥了再发，要么接收端自己自觉点，数据小于一个阈值就告诉发送端窗口此时是 0 算了，也等养肥了再告诉发送端。

发送端等着的方案就是纳格算法，这个算法相信看一下代码就知道了。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5ca35d81cfbd4f89832eb1b76007fbca~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

简单的说就是当前能发送的数据和窗口大于等于 MSS 就立即发送，否则再判断一下之前发送的包 ACK 回来没，回来再发，不然就攒数据。

接收端自觉点的方案是 David D Clark’s 方案，如果窗口数据小于某个阈值就告诉发送方窗口 0 别发，等缓过来数据大于等于 MSS 或者接受 buffer 腾出一半空间了再设置正常的 window 值给发送方。

对了提到纳格算法不得不再提一下延迟确认，纳格算法在等待接收方的确认，而开启延迟确认则会延迟发送确认，会等之后的包收到了再一起确认或者等待一段时候真的没了再回复确认。

这就相互等待了，然后延迟就很大了，两个不可同时开启。

已经有滑动窗口了为什么还要拥塞控制？
------------------

前面我已经提到了，加了拥塞控制是因为 TCP 不仅仅就管两端之间的情况，还需要知晓一下整体的网络情形，毕竟只有大家都守规矩了道路才会通畅。

前面我们提到了重传，如果不管网络整体的情况，肯定就是对方没给 ACK ，那我就无脑重传。

如果此时网络状况很差，所有的连接都这样无脑重传，是不是网络情况就更差了，更加拥堵了？

然后越拥堵越重传，一直冲冲冲！然后就 GG 了。

所以需要个拥塞控制，来避免这种情况的发送。

拥塞控制怎么搞？
--------

主要有以下几个步骤来搞：

1、慢启动，探探路。 2、拥塞避免，感觉差不多了减速看看 3、拥塞发生快速重传/恢复

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/43317b0a443649ecbe6f972e23d7c08b~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

慢启动，就是新司机上路慢慢来，初始化 cwnd（Congestion Window）为 1，然后每收到一个 ACK 就 cwnd++ 并且每过一个 RTT ，cwnd = 2\*cwnd 。

线性中带着指数，指数中又夹杂着线性增。

然后到了一个阈值，也就是 ssthresh（slow start threshold）的时候就进入了拥塞避免阶段。

这个阶段是每收到一个 ACK 就 cwnd = cwnd + 1/cwnd并且每一个 RTT 就 cwnd++。

可以看到都是线性增。

然后就是一直增，直到开始丢包的情况发生，前面已经分析到重传有两种，一种是超时重传，一种是快速重传。

如果发生超时重传的时候，那说明情况有点糟糕，于是直接把 ssthresh 置为当前 cwnd 的一半，然后 cwnd 直接变为 1，进入慢启动阶段。

如果是快速重传，那么这里有两种实现，一种是 TCP Tahoe ，和超时重传一样的处理。

一种是 TCP Reno，这个实现是把 cwnd = cwnd/2 ，然后把 ssthresh 设置为当前的 cwnd 。

然后进入快速恢复阶段，将 cwnd = cwnd + 3（因为快速重传有三次），**重传 DACK 指定的包**，如果再收到一个DACK则 cwnd++，如果收到是正常的 ACK 那么就将 cwnd 设为 ssthresh 大小，进入拥塞避免阶段。

可以看到快速恢复就重传了指定的一个包，那有可能是很多包都丢了，然后其他的包只能等待超时重传，超时重传就会导致 cwnd 减半，多次触发就指数级下降。

所以又搞了个 New Reno，多加了个 New，它是在没有SACK 的情况下改进快速恢复，它会观察重传 DACK 指定的包的响应 ACK 是否是已经发送的最大 ACK，比如你发了1、2、3、4，对方没收到 2，但是 3、4都收到了，于是你重传 2 之后 ACK 肯定是 5，说明就丢了这一个包。

不然就是还有其他包丢了，如果就丢了一个包就是之前的过程一样，如果还有其他包丢了就继续重传，直到 ACK 是全部的之后再退出快速恢复阶段。

简单的说就是一直探测到全部包都收到了再结束这个环节。

还有个 FACK，它是基于 SACK 用来作为重传过程中的拥塞控制，相对于上面的 New Reno 我们就知道它有 SACK 所以不需要一个一个试过去，具体我不展开了。

还有哪些拥塞控制算法？
-----------

从维基上看有这么多。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/00aed1e0f6234c69b283f9b2846512de~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

本来我还想哔哔几句了，哔哔了之后又删了，感觉说了和没说一样，**想深入但是实力不允许**，有点惆怅啊。

各位看官自个儿查查吧，或者等我日后修炼有成再来哔哔。

总结
--

说了这么多来总结一下吧。

TCP 是面向连接的，提供可靠、有序的传输并且还提供流控和拥塞控制，单独提取出 TCP 层而不是在 IP层实现是因为 IP 层有更多的设备需要使用，加了复杂的逻辑不划算。

三次握手主要是为了定义初始序列号为了之后的传输打下基础，四次挥手是因为 TCP 是全双工协议，因此双方都得说拜拜。

SYN 超时了就阶梯性重试，如果有 SYN攻击，可以加大半队列数，或减少重试次数，或直接拒绝。

TIME\_WAIT 是怕对方没收到最后一个 ACK，然后又发了 FIN 过来，并且也是等待处理网络上残留的数据，怕影响新连接。

TIME\_WAIT 不建议设小，或者破坏 TIME\_WAIT 机制，如果真想那么可以开启快速回收，或者重用，不过注意受益的对象。

超时重传是为了保证对端一定能收到包，快速重传是为了避免在偶尔丢包的时候需要等待超时这么长时间，SACK 是为了让发送方知道重传哪些。

D-SACK 是为了让发送方知道这次重传的原因是对方真的没收到还是自己太心急了 RTO 整小了，不至于两眼一抹黑。

滑动窗口是为了平衡发送方的发送速率和接收方的接受数率，不至于瞎发，当然还需要注意 Silly Window 的情况，同时还要注意纳格算法和延迟确认不能一起搭配。

而滑动窗口还不够，还得有个拥塞控制，因为**出行你我他，安全靠大家**，TCP 还得跳出来看看关心下当前大局势。

最后
--

至此就差不多了，不过还是有很多很多细节的，TCP 协议太复杂了，这可能是我文章里面图画的最少的一篇了，你看复杂到我图都画不来了哈哈哈。

今天我就说了个皮毛，如有纰漏请赶紧后台联系鞭挞我。

巨人的肩膀
-----

_[www.tcpipguide.com/](https://link.juejin.cn/?target=http%3A%2F%2Fwww.tcpipguide.com%2F "http://www.tcpipguide.com/")_

_[www.ionos.com/digitalguid…](https://link.juejin.cn/?target=https%3A%2F%2Fwww.ionos.com%2Fdigitalguide%2Fserver%2Fknow-how%2Fintroduction-to-tcp%2F "https://www.ionos.com/digitalguide/server/know-how/introduction-to-tcp/")_

_[www.ibm.com/developerwo…](https://link.juejin.cn/?target=https%3A%2F%2Fwww.ibm.com%2Fdeveloperworks%2Fcn%2Flinux%2Fl-tcp-sack%2F "https://www.ibm.com/developerworks/cn/linux/l-tcp-sack/")_

_[coolshell.cn/articles/11…](https://link.juejin.cn/?target=https%3A%2F%2Fcoolshell.cn%2Farticles%2F11564.html%2F "https://coolshell.cn/articles/11564.html/")_

_[tools.ietf.org/html/rfc793](https://link.juejin.cn/?target=https%3A%2F%2Ftools.ietf.org%2Fhtml%2Frfc793 "https://tools.ietf.org/html/rfc793")_

_[nmap.org/book/tcpip-…](https://link.juejin.cn/?target=https%3A%2F%2Fnmap.org%2Fbook%2Ftcpip-ref.html "https://nmap.org/book/tcpip-ref.html")_

* * *

**我是 yes，从一点点到亿点点，我们下篇见**。

# TCP RST

> 参考：
>
> - [博客园-John_ABC，Linux-TCG 出现 RST 的几种情况](https://www.cnblogs.com/JohnABC/p/6323046.html)
> - <https://segmentfault.com/a/1190000038463522>

TCP RST 引起的常见报错：

- Connection reset
- Connection reset by peer

TCP 连接和释放时还有许多细节，比如半连接状态、半关闭状态等。详情请参考这方面的巨著《TCP/IP 详解》和《UNIX 网络编程》。

前面说到出现“Connection reset”的原因是服务器关闭了 Connection\[调用了 Socket.close()方法]。大家可能有疑问了：服务器关闭了 Connection 为什么会返回“RST”而不是返回“FIN”标志。原因在于 Socket.close()方法的语义和 TCP 的“FIN”标志语义不一样：发送 TCP 的“FIN”标志表示我不再发送数据了，而 Socket.close()表示我不在发送也不接受数据了。问题就出在“我不接受数据” 上，如果此时客户端还往服务器发送数据，服务器内核接收到数据，但是发现此时 Socket 已经 close 了，则会返回“RST”标志给客户端。当然，此时客户端就会提示：“Connection reset”。详细说明可以参考 oracle 的有关文档：<http://docs.oracle.com/javase/1.5.0/docs/guide/net/articles/connection\_release.html。>

另一个可能导致的“Connection reset”的原因是服务器设置了 Socket.setLinger (true, 0)。但我检查过线上的 tomcat 配置，是没有使用该设置的，而且线上的服务器都使用了 nginx 进行反向代理，所以并不是该原因导致的。关于该原因上面的 oracle 文档也谈到了并给出了解释。

此外啰嗦一下，另外还有一种比较常见的错误“Connection reset by peer”，该错误和“Connection reset”是有区别的：

- 服务器返回了“RST”时，如果此时客户端正在从 Socket 套接字的输出流中读数据则会提示 Connection reset”；
- 服务器返回了“RST”时，如果此时客户端正在往 Socket 套接字的输入流中写数据则会提示“Connection reset by peer”。

“Connection reset by peer”如下图所示：

前面谈到了导致“Connection reset”的原因，而具体的解决方案有如下几种：

出错了重试；

客户端和服务器统一使用 TCP 长连接；

客户端和服务器统一使用 TCP 短连接。

首先是出错了重试：这种方案可以简单防止“Connection reset”错误，然后如果服务不是“幂等”的则不能使用该方法；比如提交订单操作就不是幂等的，如果使用重试则可能造成重复提单。

然后是客户端和服务器统一使用 TCP 长连接：客户端使用 TCP 长连接很容易配置（直接设置 HttpClient 就好），而服务器配置长连接就比较麻烦了，就拿 tomcat 来说，需要设置 tomcat 的 maxKeepAliveRequests、connectionTimeout 等参数。另外如果使用了 nginx 进行反向代理或负载均衡，此时也需要配置 nginx 以支持长连接（nginx 默认是对客户端使用长连接，对服务器使用短连接）。

使用长连接可以避免每次建立 TCP 连接的三次握手而节约一定的时间，但是我这边由于是内网，客户端和服务器的 3 次握手很快，大约只需 1ms。ping 一下大约 0.93ms（一次往返）；三次握手也是一次往返（第三次握手不用返回）。根据 80/20 原理，1ms 可以忽略不计；又考虑到长连接的扩展性不如短连接好、修改 nginx 和 tomcat 的配置代价很大（所有后台服务都需要修改）；所以这里并没有使用长连接。

正常情况 tcp 四层握手关闭连接，rst 基本都是异常情况，整理如下： 

0. 使用 ping 可以看到丢包情况
1. GFW
2. 对方端口未打开，发生在连接建立。
如果对方 sync_backlog 满了的话，sync 简单被丢弃，表现为超时，而不会 rst
3. close Socket 时 recv buffer 不为空。
例如，客户端发了两个请求，服务器只从 buffer 读取第一个请求处理完就关闭连接，tcp 层认为数据没有正确提交到应用，使用 rst 关闭连接。
3. 移动链路。
移动网络下，国内是有 5 分钟后就回收信令，也就是 IM 产品，如果心跳>5 分钟后服务器再给客户端发消息，就会收到 rst。也要查移动网络下 IM 保持<5min 心跳。
4. 负载等设备。
负载设备需要维护连接转发策略，长时间无流量，连接也会被清除，而且很多都不告诉两层机器，新的包过来时才通告 rst。
Apple push 服务也有这个问题，而且是不可预期的偶发性连接被 rst；rst 前第一个消息 write 是成功的，而第二条写才会告诉你连接被重置，
曾经被它折腾没辙，因此打开每 2 秒一次 tcp keepalive，固定 5 分钟 tcp 连接回收，而且发现连接出错时，重发之前 10s 内消息。
5. SO_LINGER 应用强制使用 rst 关闭
该选项会直接丢弃未发送完毕的 send buffer，可能造成业务错误，慎用； 当然内网服务间 http client 在收到应该时主动关闭，使用改选项，会节省资源。
好像曾经测试过 haproxy 某种配置下，会使用 rst 关闭连接，少了网络交互而且没有 TIME_WAIT 问题
6. 超过超时重传次数、网络暂时不可达
7. TIME_WAIT 状态。
tw_recycle = 1 时，sync timestamps 比上次小时，会被 rst
7. 设置 connect_timeout。
应用设置了连接超时，sync 未完成时超时了，会发送 rst 终止连接。
8. 非正常包。
连接已经关闭，seq 不正确等
9. keepalive 超时。
公网服务 tcp keepalive 最好别打开；移动网络下会增加网络负担，切容易掉线；非移动网络核心 ISP 设备也不一定都支持 keepalive，曾经也发现过广州那边有个核心节点就不支持。
10. 数据错误，不是按照既定序列号发送数据 11.在一个已关闭的 socket 上接收数据 12.服务器关闭或异常终止了连接，由于网络问题，客户端没有收到服务器的关闭请求，这称为 TCP 半打开连接。就算重启服务器，也没有连接信息。如果客户端向提其写入数据，对方就会回应一个 RST 报文段。

# 三次握手与四次挥手异常处理

> 参考：
>
> - 原文链接：[公众号-小林 coding，TCP 才不傻！](https://mp.weixin.qq.com/s/CTqag_TxAHLuUwLvbrmlnw)

大家好，我是小林。

之前收到个读者的问题，对于 TCP 三次握手和四次挥手的一些疑问：

- 第一次握手，如果客户端发送的 SYN 一直都传不到被服务器，那么客户端是一直重发 SYN 到永久吗？客户端停止重发 SYN 的时机是什么？
- 第三次握手，如果服务器永远不会收到 ACK，服务器就永远都留在 Syn-Recv 状态了吗？退出此状态的时机是什么？
- 第三次挥手，如果客户端永远收不到 FIN,ACK，客户端永远停留在 Fin-Wait-2 状态了吗？退出此状态时机是什么时候呢？
- 第四次挥手，如果服务器永远收不到 ACK，服务器永远停留在 Last-Ack 状态了吗？退出此状态的时机是什么呢？
- 如果客户端 在 2SML 内依旧没收到 FIN,ACK，会关闭链接吗？服务器那边怎么办呢，是怎么关闭链接的呢？

可以看到，这些问题都是关于 **TCP 是如何处理这些异常场景**的，我们在学 TCP 连接建立和断开的时候，总是以为这些过程能如期完成。

**可惜理想很丰满，现实很骨感，事实预料呀**。

**TCP 当然不傻**，对以上这些异常场景都是有做处理的。

当时也用做实验的方式带大家看 TCP 是如何处理这些异常场景的。

> 如果新读者还不知道小林的图解网络 PDF，可以到我公众号后台回复「图解」获取就行。

不过，当时这些知识分散到了多个章节，这次就针对读者问的这一系列问题，来详细说说 TCP 是怎么处理这些异常的？

这些异常场景共分为两大类，第一类是 TCP 三次握手期间的异常，第二类是 TCP 四次挥手期间的异常。

### TCP 三次握手期间的异常

我们先来看看 TCP 三次握手是怎样的。

#### 第一次握手丢失了，会发生什么？

当客户端想和服务端建立 TCP 连接的时候，首先第一个发的就是 SYN 报文，然后进入到 `SYN_SENT` 状态。

在这之后，如果客户端迟迟收不到服务端的 SYN-ACK 报文（第二次握手），就会触发超时重传机制。

不同版本的操作系统可能超时时间不同，有的 1 秒的，也有 3 秒的，这个超时时间是写死在内核里的，如果想要更改则需要重新编译内核，比较麻烦。

当客户端在 1 秒后没收到服务端的 SYN-ACK 报文后，客户端就会重发 SYN 报文，那到底重发几次呢？

在 Linux 里，客户端的 SYN 报文最大重传次数由 `tcp_syn_retries`内核参数控制，这个参数是可以自定义的，默认值一般是 5。

通常，第一次超时重传是在 1 秒后，第二次超时重传是在 2 秒，第三次超时重传是在 4 秒后，第四次超时重传是在 8 秒后，第五次是在超时重传 16 秒后。没错，**每次超时的时间是上一次的 2 倍**。

当第五次超时重传后，会继续等待 32 秒，如果服务端仍然没有回应 ACK，客户端就不再发送 SYN 包，然后断开 TCP 连接。

所以，总耗时是 1+2+4+8+16+32=63 秒，大约 1 分钟左右。

#### 第二次握手丢失了，会发生什么？

当服务端收到客户端的第一次握手后，就会回 SYN-ACK 报文给客户端，这个就是第二次握手，此时服务端会进入 `SYN_RCVD` 状态。

第二次握手的 `SYN-ACK` 报文其实有两个目的 ：

- 第二次握手里的 ACK， 是对第一次握手的确认报文；
- 第二次握手里的 SYN，是服务端发起建立 TCP 连接的报文；

所以，如果第二次握手丢了，就会发送比较有意思的事情，具体会怎么样呢？

因为第二次握手报文里是包含对客户端的第一次握手的 ACK 确认报文，所以，如果客户端迟迟没有收到第二次握手，那么客户端就觉得可能自己的 SYN 报文（第一次握手）丢失了，于是**客户端就会触发超时重传机制，重传 SYN 报文**。

然后，因为第二次握手中包含服务端的 SYN 报文，所以当客户端收到后，需要给服务端发送 ACK 确认报文（第三次握手），服务端才会认为该 SYN 报文被客户端收到了。

那么，如果第二次握手丢失了，服务端就收不到第三次握手，于是**服务端这边会触发超时重传机制，重传 SYN-ACK 报文**。

在 Linux 下，SYN-ACK 报文的最大重传次数由 `tcp_synack_retries`内核参数决定，默认值是 5。

因此，当第二次握手丢失了，客户端和服务端都会重传：

- 客户端会重传 SYN 报文，也就是第一次握手，最大重传次数由 `tcp_syn_retries`内核参数决定。；
- 服务端会重传 SYN-AKC 报文，也就是第二次握手，最大重传次数由 `tcp_synack_retries`   内核参数决定。

#### 第三次握手丢失了，会发生什么？

客户端收到服务端的 SYN-ACK 报文后，就会给服务端回一个 ACK 报文，也就是第三次握手，此时客户端状态进入到 `ESTABLISH` 状态。

因为这个第三次握手的 ACK 是对第二次握手的 SYN 的确认报文，所以当第三次握手丢失了，如果服务端那一方迟迟收不到这个确认报文，就会触发超时重传机制，重传 SYN-ACK 报文，直到收到第三次握手，或者达到最大重传次数。

注意，**ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文**。

### TCP 四次挥手期间的异常

我们再来看看 TCP 四次挥手的过程。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/24e305c7-9ba8-4113-9be1-44404948d366/640)

#### 第一次挥手丢失了，会发生什么？

当客户端（主动关闭方）调用 close 函数后，就会向服务端发送 FIN 报文，试图与服务端断开连接，此时客户端的连接进入到 `FIN_WAIT_1` 状态。

正常情况下，如果能及时收到服务端（被动关闭方）的 ACK，则会很快变为 `FIN_WAIT2` 状态。

如果第一次挥手丢失了，那么客户端迟迟收不到被动方的 ACK 的话，也就会触发超时重传机制，重传 FIN 报文，重发次数由 `tcp_orphan_retries` 参数控制。

当客户端重传 FIN 报文的次数超过 `tcp_orphan_retries`   后，就不再发送 FIN 报文，直接进入到 `close` 状态。

#### 第二次挥手丢失了，会发生什么？

当服务端收到客户端的第一次挥手后，就会先回一个 ACK 确认报文，此时服务端的连接进入到 `CLOSE_WAIT` 状态。

在前面我们也提了，ACK 报文是不会重传的，所以如果服务端的第二次挥手丢失了，客户端就会触发超时重传机制，重传 FIN 报文，直到收到服务端的第二次挥手，或者达到最大的重传次数。

这里提一下，当客户端收到第二次挥手，也就是收到服务端发送的 ACK 报文后，客户端就会处于 `FIN_WAIT2` 状态，在这个状态需要等服务端发送第三次挥手，也就是服务端的 FIN 报文。

对于 close 函数关闭的连接，由于无法再发送和接收数据，所以`FIN_WAIT2` 状态不可以持续太久，而  `tcp_fin_timeout` 控制了这个状态下连接的持续时长，默认值是 60 秒。

这意味着对于调用 close 关闭的连接，如果在 60 秒后还没有收到 FIN 报文，客户端（主动关闭方）的连接就会直接关闭。

#### 第三次挥手丢失了，会发生什么？

当服务端（被动关闭方）收到客户端（主动关闭方）的 FIN 报文后，内核会自动回复 ACK，同时连接处于 `CLOSE_WAIT` 状态，顾名思义，它表示等待应用进程调用 close 函数关闭连接。

此时，内核是没有权利替代进程关闭连接，必须由进程主动调用 close 函数来触发服务端发送 FIN 报文。

服务端处于 CLOSE_WAIT 状态时，调用了 close 函数，内核就会发出 FIN 报文，同时连接进入 LAST_ACK 状态，等待客户端返回 ACK 来确认连接关闭。

如果迟迟收不到这个 ACK，服务端就会重发 FIN 报文，重发次数仍然由 `tcp_orphan_retrie`s 参数控制，这与客户端重发 FIN 报文的重传次数控制方式是一样的。

#### 第四次挥手丢失了，会发生什么？

当客户端收到服务端的第三次挥手的 FIN 报文后，就会回 ACK 报文，也就是第四次挥手，此时客户端连接进入 `TIME_WAIT` 状态。

在 Linux 系统，TIME_WAIT 状态会持续 60 秒后才会进入关闭状态。

然后，服务端（被动关闭方）没有收到 ACK 报文前，还是处于 LAST_ACK 状态。

如果第四次挥手的 ACK 报文没有到达服务端，服务端就会重发 FIN 报文，重发次数仍然由前面介绍过的 `tcp_orphan_retries` 参数控制。

---

是吧，TCP 聪明着很！
