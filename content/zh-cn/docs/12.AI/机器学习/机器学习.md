---
title: "机器学习"
linkTitle: "机器学习"
weight: 1
---

# 概述

> 参考：
>
> - [Wiki, Machine_learning](https://en.wikipedia.org/wiki/Machine_learning)
> - [Wiki, Data mining](https://en.wikipedia.org/wiki/Data_mining)

**Machine learning(机器学习，简称 ML)** 是一个致力于理解和构建让机器“学习”的方法的领域 - 就是利用数据提高计算机在某些任务上性能的方法。

就像人需要学习一样，机器也需要学习。机器学习是[人工智能](/docs/12.AI/12.AI.md)的一个广泛子领域。

机器学习通常需要一个模型和训练模型的方法，通过训练好的模型，可以更准确得预测或分类新的未见过的数据。e.g. 自然语言处理中，当我们向模型提交输入的文字后，模型会根据参数，逐步预测将要输出的每一个文字，每个文字都是由模型根据训练好的参数，依据统计学的百分比概率推导出来的。

我们人类好像也是通过这种方式沟通的，每个字都在脑子中依据已有知识，推理出来后，逐一说出来。

TODO: 这都是啥？

- Convolutional Neural Network(卷积神经网络简称 CNN)
- Transformer(变压器)
- Generative Adversarial Network(对抗网络模型，简称 GAN)
- **Data mining(数据挖掘)**

# 学习方法

> 参考：
>
> - [Wiki, Machine_learning - Approaches(方法)](https://en.wikipedia.org/wiki/Machine_learning#Approaches)
> - [Wiki, Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) 监督学习
> - [Wiki, Unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning) 无监督学习

TODO:

- RLHF # Reinforcement Learning from Human Feedback(基于人类反馈的强化学习)
    - [B 站 - AI老兵文哲，大模型成功背后的RLHF到底是什么？](https://www.bilibili.com/video/BV1Cr421K7xt)
    - 个人理解：有点像强化学习，但是对完成目标提供的奖励不是提前设计好的。这个奖励通常是一个评分体系，由人类预先训练模型（假如称为 Mentor）进行的打分。然后让待训练的模型，被整个 Mentor 打分，直到这个待训练的模型可以获得最高的分数。

机器学习方法传统上分为三类，它们对应于学习范式，取决于学习系统可用的“信号”或“反馈”的性质：

- **Supervised learning(监督学习)** # 使用有标注的数据集 ——> 学习输入的数据和输出的数据之间的关系 ——> 预测新数据
- **Unsupervised learning(无监督学习)** # 使用无标注的数据集 ——> 学习数据中的内在结构并分类 ——> 可以判断新数据属于哪类
- **Reinforcement learning(强化学习)** # 设定规则与目标，以及完成目标的奖励 ——> 学习如何在规则内完成目标并获取奖励，不断尝试 ——> 可以对相同目标执行最佳策略

监督学习:

- 训练数据包含输入特征和对应的标签/答案
- 例如：图片分类中,每张图片都标注了"猫"或"狗" 
- 算法目标是学习输入到标签的映射关系
- 常见应用：分类、回归预测
- 最终目标是：给一张新图片，能准确判断是猫还是狗

无监督学习:

- 训练数据只有输入特征，没有标签，训练数据是 1000 张动物图片，但没有任何标注
- 模型会基于图片的相似度(如颜色分布、形状特征等)自动分组
- 最终可能会将图片分成几类:
    - 一类都是毛茸茸、有尖耳朵的(实际上是猫，但是模型并不知道这叫 ”猫“，文字只是人类的定义)
    - 一类都是体型较大、有长嘴的(实际上是狗，但是模型并不知道这叫 ”狗“，文字只是人类的定义)
    - 但模型并不知道这些类别的具体含义
- 算法目标是发现数据中的内在结构和模式
- 常见应用：聚类分析、降维、异常检测
- 最终目标是：给一张新图片，能够判断它与哪组已有图片最相似，将它自动归类到相应的组别中

强化学习：

- 训练过程中没有预设的标准答案，而是通过与环境交互获得奖励反馈
- 例如：训练一个游戏 AI，开始时完全不知道游戏规则
- 智能体（Agent）在游戏中随机尝试各种操作：
    - 做出好的动作 → 获得正奖励（如得分、过关）
    - 做出坏的动作 → 获得负奖励（如扣分、死亡）
- 通过不断试错，智能体逐渐学会：
    - 哪些行为能带来更多奖励
    - 如何制定长期策略来最大化总收益
- 算法目标是学习最优的行为策略（Policy）
- 常见应用：游戏AI、机器人控制、自动驾驶、推荐系统
- 最终目标是：在任何游戏状态下，都能选择最优的行动来获得最高分数

简单来说:

- 监督学习像是有老师指导的学习，告诉你答案是什么，需要自己**总结**数据中的规律
- 无监督学习像是自学，需要自己**发现**数据中的规律
- 强化学习像是通过实践学习，在做中学，通过成功和失败的**经验**积累最优策略

# 模型

机器学习 通常包含 [Model](/docs/12.AI/机器学习/Model.md)(模型) 的训练

模型通常可以应用于 1 种或多种 Tasks(任务)。

# 任务

**Tasks(任务)** 通常是指一种具体的 AI 应用场景，比如 计算机视觉 的 目标检测、自然语言处理 的 情感分析、etc. 这些都属于 Task。有的地方将 Tasks 称为 Sub-domains(子领域)

Claude 问题: 常见的 AI 任务有哪些？

回答:

以下是一些常见的AI任务:

- 自然语言处理
  - 文本分类
  - 情感分析
  - 机器翻译
  - 问答系统
- 计算机视觉
  - 图像分类
  - 目标检测
  - 人脸识别
- 语音技术
  - 语音识别
  - 语音合成
- 推荐系统
- 预测分析
- 自动驾驶
- 游戏AI
- 异常检测
- 机器人控制

这些只是AI应用的一部分。AI技术正在不断发展,应用领域也在持续扩大。您对其中哪个领域特别感兴趣?我可以提供更多相关信息。

# 数学与机器学习

根据 [知乎，用人话讲明白线性回归LinearRegression](https://zhuanlan.zhihu.com/p/72513104) 的学习。得知：回归是”倒推“。那么根据一组数据”倒推“出其中的关系，这些关系也就是模型。那堆离散的点就像训练时的数据集，那条直线就是这组数据的模型。

所以，是否可以说，AI 的部分训练方式（有/无监督学习）本质就是 回归 问题 i.e. 倒推？

训练时使用的数据，可以在现阶段简单得看作为线性回归中，一个个的数据点。

此时，来到了损失问题。为什么描述一组数据的就一定是那条直线，而不能是这条直线？所以需要找到预测值与真实值之间的差距。**损失函数是衡量这条直线误差的函数。这个函数的值越小，说明直线越能拟合我们的数据。**

线性回归是**对一个或多个自变量之间关系进行建模**的方法