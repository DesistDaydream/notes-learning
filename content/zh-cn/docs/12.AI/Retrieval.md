---
title: "Retrieval"
linkTitle: "Retrieval"
weight: 20
---

# 概述

> 参考：
>
> -

# RAG

> 参考：
>
> - [Wiki, Retrieval-augmented generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)
> - [B 站，AI知识库RAG技术原理，三大痛点与进阶方案【不用编程】](https://www.bilibili.com/video/BV1NMoFYoEsb)

**Retrieval-augmented generation(检索增强生成，简称 RAG)** 是一种使 [12.AI](/docs/12.AI/12.AI.md) 模型能够检索信息的技术。它修改了与 LLM 的交互，使模型能够参考指定的一组文档来响应用户的查询，并使用这些信息来补充其预先存在训练数据中的信息。

用户把资料添加进知识库的时候，程序会先把它们拆分成很多个文本块，然后使用嵌入模型对这些文本块进行向量化（向量化指的是把切拆分后的文本），变成一个超长的数字序列。然后程序把向量以及对应的文本保存在向量数据库里面。

接下来用户开始提问，不过这个提问并非直接送达到大模型那里，而是把其本身也经过向量化处理，先变成一个1024维的向量。然后把用户的提问与向量数据库进行相似度匹配，这个匹配过程是基于向量的纯数学运算，最后知识库**选出匹配度最高**的几个**原文片段**，再加上用户的问题发给大模型，大模型进行最后的**归纳总结**

存在问题：

- 切片很粗暴
- 检索不精准 # 搜索知识库时，只能找到切片，无法将搜索内容与全文进行上下文管理，只有部分切片。最后会 AI 拿到的内容是不足的，导致结果不精准。
- 没有大局观

**重排序模型**，可以把向量数据库初步检索出来的数据，使用专用的重排序模型进行更深入的语义分析。然后再按照问题的相关性进行重新的排序，把相关性最大的一些数据排到前面并且交付给大模型。这是一种先粗后细的两步检索策略，可以进一步提高检索精度

使用超长上下文，避免切片太碎，但是。。。。资源消耗非常非常高。。。。
