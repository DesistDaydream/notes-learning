---
title: 编码与解码
linkTitle: 编码与解码
date: 2023-11-17T11:30
weight: 1
---

# 概述

> 参考：
>
> - [Wiki，Code](https://en.wikipedia.org/wiki/Code)

在通信和信息处理中，**Code(代码)** 是一种规则系统，用于将信息（e.g. 字母、单词、声音、图像、手势、etc.）以另一种形式展示（e.g. 数字、短码、加密、etc.），以便可以通过通信通道或存储在存储器中进行通信。

> 注意与 [编程](docs/2.编程/编程.md) 中的 Code 概念区分。虽然编程中的 Code 也有类似将一种语言转成另一种语言的概念。但是与这里的 Code 概念并不完全一样。

**Encoding(编码)** 是信息从一种形式或格式,转换为另一种形式或格式的过程。而 **Decoding(解码)** 则是编码的逆过程。

首先需要明确一点，计算机的组件处理信息都是基于 **二进制** 来实现的，也就是说。都是 0 和 1 这种信息，不管电路的一开一关，灯的一亮一灭，都可以用 0 和 1 来表示。而计算机也是通过 0 和 1 来存储以及处理数据的。

**但是计算机在使用的时候，我们看到并不是只有 0 和 1，我们打字也不是只输入 0 和 1，不但有文字，还有图片、视频、音频等等。这是为什么呢？**

由于计算机只认识 0 和 1 这种二进制数据，其他格式的数据都需要转换成二进制才能被计算机处理，也就是说我们在计算机上看到的文本、视频、可执行程序等格式的文件，最终都会进行编码以转换成二进制数据交给计算机处理；而计算机处理完成后，会进行解码以将二进制数据转换为 文本、图片、视频 等格式，以便人类可以查看。这种来回转换的过程，就称为 **Encoding(编码)** 与 **Decoding(解码)**。

总的来说，通常编码与解码是相对人类来说，把人类当做主体，i.e. 人类是数据的发送者和接收者。此时，Encoding 是将来自发送者的信息转换为用于通信和存储的符号。Decoding 是将代码符号转换回接收者可以理解的形式。

而文字、图片、音/视频 等等数据，其编码方式又各不相同。

- 字符编码 # 用来对文字进行 编/解码 的过程
- 图形编码 # 对图片进行 编/解码 的过程
- 音频编码 # 对音频进行 编/解码 的过程

当然，随着时代的发展，Encoding(编码) 与 Decoding(解码) 也不仅仅代指上述三种类型，就如概述中的第一句话，**任何格式的数据转换成另一种，都可以称之为编码！**比如 Base64 编码，则是可以将二进制数据，转换为字符串数据，这种行为，也可以称之为编码。

> 这里面的**编码和解码**从某种角度看存在**相对概念**，上文描述的编码和解码，是对于人类与计算机而言。假如将人类与计算机变为 Go 编程语言与 JSON 数据，那么对于 Go 来说，其本身其实并不认识 JSON 格式数据，所以如果 Go 程序当做主体
>
> - 作为发送者，将 Go 程序自身可以理解的数据（比如 struct、slice 等等）发送到 json 库，进行编码后，转换为供人类用于通信和存储的 JSON 符号数据。
> - 作为接收者，由 json 库解码后，转为 Go 程序自身可以理解的数据（比如 struct、slice 等等）。
>
> 这段内容要是理解不了，可以先学习编程语言的各种 JSON、YAML 数据处理库，再回头看。

## 编码解码与协议

> 参考：
>
> - [公众号-码农的荒岛求生，神奇的 Google 二进制编解码技术：Protobuf](https://mp.weixin.qq.com/s/kfyTcs6xuDvlCX3U04Edng)

计算机网络编程中一个非常基本的问题：该怎样表示 client 与 server 之间交互的数据，在往下看之前先想一想这个问题。

### 共识与协议

这个问题可不像看上去的那样简单，因为 client 进程和 server 进程运行在不同的机器上，这些机器可能运行在不同的处理器平台、可能运行在不同的操作系统、可能是由不同的编程语言编写的，server 要怎样才能识别出 client 发送的是什么数据呢？就像这样：

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/rpxf9d/1663061154712-3689268d-a902-4892-93f8-6682f9329d0a.png)

client 给 server 发送了一段数据：

`0101000100100001`

server 怎么能知道该怎样“解读”这段数据呢？

显然，client 和 server 在发送数据之前必须首先达成某种关于怎样解读数据的共识，这就是所谓的**协议**。

这里的协议可以是这样的：“将每 8 个比特为一个单位解释为无符号数字”，如果协议是这样的，那么 server 接收到这串二进制后就会将其解析为 81(01010001)与 33(00100001)。

当然，这里的协议也可以是这样的：“将每 8 个比特为一个单位解释为 ASCII 字符”，那么 server 接收到这串二进制后就将其解析为“Q!”。

可见，同样一串二进制在不同的“上下文/协议”下有完全不一样的解读，**这也是为什么计算机明明只认知 0 和 1 但是却能处理非常复杂任务的根本原因，因为一切都可以编码为 0 和 1，同样的我们也可以从 0 和 1 中解析出我们想要的信息，这就是所谓的编解码技术。**

实际上不止 0 和 1，我们也可以将信息编码为摩斯密码(Morse code)等，只不过计算机擅长处理 0 和 1 而已。

![](https://notes-learning.oss-cn-beijing.aliyuncs.com/rpxf9d/1663061154726-975db7c1-2b96-429d-a007-12706a83be5f.png)

# 字符编码

详见：[字符的编码与解码](/docs/8.通用技术/编码与解码/字符的编码与解码/字符的编码与解码.md)

# 图形编码

详见：[图像的编码与解码](/docs/8.通用技术/编码与解码/图像的编码与解码.md)

# 音频编码

详见：[音频的编码与解码](/docs/8.通用技术/编码与解码/音频的编码与解码.md)
